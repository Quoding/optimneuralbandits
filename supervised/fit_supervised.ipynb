{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from fit_sup_utils import *\n",
    "from networks import Network, VariableNet\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # default = (6.4, 4.8)\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 140  # default = 100\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "title_font_size = \"10\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 1.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What affects overfitting here ?\n",
    "* Optimizer - Adam fits faster and more than SGD\n",
    "* Learning rate - Lower (0.001) seems better than default (1e-2)\n",
    "* Batch size - Lower batch size seems to lead to more overfitting. Larger ones seem to average out extremes in the input dataset during backprop\n",
    "* Net width - Obviously\n",
    "* Hidden layers - Obviously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What helps fitting \"outliers\" ?\n",
    "Fitting is meant in a broad sense here and just means \"Not predicting the mean\"\n",
    "\n",
    "* Lower LR (?) 0.001 is better than 0.01\n",
    "* Label dist. smoothing, but seems to affect validation perf for the \"common\" cluster (not outlier)\n",
    "* Quantile loss seems to help over estimating values properly when using a quantile at 0.75 (i.e. low risk cluster is overestimated to a lesser extent than high risk cluster, which could be good for the bandit algorithm), HOWEVER, fitting a quantile that isn't 50 feels like it could mess up with NeuralTS (since we're not predicting a mean anymore, we're predicting a quantile, and the output of the NN usually goes into a Normal distribution as the mean param)\n",
    "* Label dist smoothing with a bigger batch size seems to help \n",
    "* More data. You need to have at least a couple of high risk observations in order to get a good idea of what makes a high risk combination\n",
    "* for 1000 rx, small batches, sqrt_inv LDS, smaller models seems to have less bias in validation\n",
    "* Low dim, MSE is sufficient, high dim, quantile seems better in early \n",
    "* 3 layers of 128 width seem to be sufficient to overfit in training\n",
    "* lr of 0.001 seem to help that overfitting, nice, lr of 0.01 seems to fail overfitting in some cases (dataset 100 and 50)\n",
    "* mse is better than rmse\n",
    "* If doing LDS, sqrt_inv is better than just True\n",
    "* For low dim data: MSE seems equal or better than quantile in more situations (learning rate for example)\n",
    "* Avoir un validation set pondere de maniere plus egale (ex. bins et extrema)\n",
    "\n",
    "* **LDS WORKS, IMPORTANT, USE IT**\n",
    "* **EXTREMA VALIDATION IS BETTER THAN BINS**\n",
    "* **WITH VAL EXTREMA, MSE PERFORMS SIMILARLY TO QUANTILE, BUT IS SIMPLER**\n",
    "* **IF WE USE WEIGHT DECAY THAT DECREASES WHEN EPOCHS GO UP, FOCUS ON TRAINING LOSS, WE CAN GET VERY GOOD RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tuner_top10.csv\")\n",
    "config_cols = [col for col in df.columns if 'config' in col]\n",
    "config_df = df[config_cols]\n",
    "mapper = {col: col.split(\"/\")[1] for col in config_cols}\n",
    "config_df = config_df.rename(columns=mapper)\n",
    "config_df = config_df.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config/batch_norm</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/dataset</th>\n",
       "      <th>config/decay</th>\n",
       "      <th>config/hidden</th>\n",
       "      <th>config/lds</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/n_obs</th>\n",
       "      <th>config/optim</th>\n",
       "      <th>config/patience</th>\n",
       "      <th>config/validation</th>\n",
       "      <th>config/width</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>plateau</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   config/batch_norm  config/batch_size                       config/dataset  \\\n",
       "0               True               1024  500_rx_100000_combis_10_patterns_23   \n",
       "1               True               1024  500_rx_100000_combis_10_patterns_23   \n",
       "2               True               1024  500_rx_100000_combis_10_patterns_23   \n",
       "3               True               1024  500_rx_100000_combis_10_patterns_23   \n",
       "4               True                512  500_rx_100000_combis_10_patterns_23   \n",
       "5               True                512  500_rx_100000_combis_10_patterns_23   \n",
       "6               True                512  500_rx_100000_combis_10_patterns_23   \n",
       "7               True                512  500_rx_100000_combis_10_patterns_23   \n",
       "8               True                256  500_rx_100000_combis_10_patterns_23   \n",
       "9               True                256  500_rx_100000_combis_10_patterns_23   \n",
       "\n",
       "   config/decay  config/hidden  config/lds config/lr  config/n_obs  \\\n",
       "0       0.00001              1         NaN       0.1         20000   \n",
       "1       0.00001              1         NaN       0.1         20000   \n",
       "2       0.00001              1         NaN       0.1         20000   \n",
       "3       0.00010              1         NaN       0.1         20000   \n",
       "4       0.00100              1         NaN   plateau         20000   \n",
       "5       0.00010              1         NaN       0.1         20000   \n",
       "6       0.00010              1         NaN       0.1         20000   \n",
       "7       0.00010              1         NaN       0.1         20000   \n",
       "8       0.00001              1         NaN       0.1         20000   \n",
       "9       0.00001              1         NaN       0.1         20000   \n",
       "\n",
       "  config/optim  config/patience  config/validation  config/width  test_loss  \n",
       "0         adam               10                NaN           128   0.004900  \n",
       "1         adam               50                NaN           128   0.004900  \n",
       "2         adam               25                NaN           128   0.004900  \n",
       "3         adam               10                NaN           128   0.005309  \n",
       "4         adam               50                NaN           128   0.005350  \n",
       "5         adam               10                NaN           128   0.005436  \n",
       "6         adam               25                NaN           128   0.005436  \n",
       "7         adam               50                NaN           128   0.005436  \n",
       "8         adam               10                NaN           128   0.005438  \n",
       "9         adam               25                NaN           128   0.005438  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[config_cols + [\"test_loss\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_norm</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dataset</th>\n",
       "      <th>decay</th>\n",
       "      <th>hidden</th>\n",
       "      <th>lds</th>\n",
       "      <th>lr</th>\n",
       "      <th>n_obs</th>\n",
       "      <th>optim</th>\n",
       "      <th>patience</th>\n",
       "      <th>validation</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1024</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>plateau</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>256</td>\n",
       "      <td>500_rx_100000_combis_10_patterns_23</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20000</td>\n",
       "      <td>adam</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_norm  batch_size                              dataset    decay  \\\n",
       "0        True        1024  500_rx_100000_combis_10_patterns_23  0.00001   \n",
       "1        True        1024  500_rx_100000_combis_10_patterns_23  0.00001   \n",
       "2        True        1024  500_rx_100000_combis_10_patterns_23  0.00001   \n",
       "3        True        1024  500_rx_100000_combis_10_patterns_23   0.0001   \n",
       "4        True         512  500_rx_100000_combis_10_patterns_23    0.001   \n",
       "5        True         512  500_rx_100000_combis_10_patterns_23   0.0001   \n",
       "6        True         512  500_rx_100000_combis_10_patterns_23   0.0001   \n",
       "7        True         512  500_rx_100000_combis_10_patterns_23   0.0001   \n",
       "8        True         256  500_rx_100000_combis_10_patterns_23  0.00001   \n",
       "9        True         256  500_rx_100000_combis_10_patterns_23  0.00001   \n",
       "\n",
       "   hidden   lds       lr  n_obs optim  patience validation  width  \n",
       "0       1  None      0.1  20000  adam        10       None    128  \n",
       "1       1  None      0.1  20000  adam        50       None    128  \n",
       "2       1  None      0.1  20000  adam        25       None    128  \n",
       "3       1  None      0.1  20000  adam        10       None    128  \n",
       "4       1  None  plateau  20000  adam        50       None    128  \n",
       "5       1  None      0.1  20000  adam        10       None    128  \n",
       "6       1  None      0.1  20000  adam        25       None    128  \n",
       "7       1  None      0.1  20000  adam        50       None    128  \n",
       "8       1  None      0.1  20000  adam        10       None    128  \n",
       "9       1  None      0.1  20000  adam        25       None    128  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "for i, row in config_df.iterrows():\n",
    "    d = row.to_dict()\n",
    "    d[\"custom_layers\"] = None\n",
    "    d[\"dropout_rate\"] = None\n",
    "    d[\"classif_thresh\"] = None\n",
    "    configs.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "# seeds = list(range(25))\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "param_values = {\n",
    "    \"dataset\": [\n",
    "        # \"50_rx_100000_combis_4_patterns_3\",\n",
    "        # \"100_rx_100000_combis_10_patterns_35\",\n",
    "        \"500_rx_100000_combis_10_patterns_23\",\n",
    "    ],\n",
    "    \"width\": [128],\n",
    "    \"hidden\": [1],\n",
    "    \"n_obs\": [20000],\n",
    "    # \"n_obs\": [100, 1000, 10000, 20000],\n",
    "    \"decay\": [0.0001],\n",
    "    # \"decay\": [0.00001, 0.0001, 0.001, 0.01, 0.1, \"epoch\"],\n",
    "    \"lr\": [\"plateau\"],\n",
    "    # \"lr\": [\"plateau\", 0.001, 0.01, 0.1],\n",
    "    \"custom_layers\": [None],\n",
    "    \"lds\": [True],\n",
    "    # \"lds\": [True, None],\n",
    "    \"batch_size\": [128],\n",
    "    # \"batch_size\": [32, 64, 128, 256, 512, 1024],\n",
    "    \"dropout_rate\": [None],\n",
    "    \"loss\": [[\"mse\"]],\n",
    "    \"classif_thresh\": [None],\n",
    "    \"batch_norm\": [False],\n",
    "    # \"batch_norm\": [False, True],\n",
    "    \"patience\": [50],\n",
    "    # \"patience\": [100, 50, 25, 10],\n",
    "    \"validation\": [None],\n",
    "    # \"validation\": [None, \"bins\", \"extrema\"],\n",
    "    \"optim\": [\"adam\"],\n",
    "    # \"optim\": [\"adam\", \"sgd\"],\n",
    "    \"modif\": [\"first_tune_res\"],\n",
    "}\n",
    "\n",
    "# METHO: Prendre modele qui fonctionne bien, le decomposer\n",
    "\n",
    "# TODO\n",
    "# Decay = 0.1, 0.01, 0.001, 0.0001 DONE\n",
    "# lr = 0.001, 0.01, 0.1, \"plateau\", optim = \"adam\", \"sgd\" avec les lrs DONE\n",
    "# lds = True, False DONE\n",
    "# batch_size = 32, 64, 128, 256, 512, 1024 DONE\n",
    "# batch_norm = False, True  DONE\n",
    "# early_stop = patience = 100, 50, 25, 10 validation = None, \"bins\", \"extrema\" DONE\n",
    "# n_obs = 100, 1000, 10000, 20000 DONE\n",
    "\n",
    "configs = [dict(zip(param_values, v)) for v in product(*param_values.values())]\n",
    "print(len(configs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(config, exp_dir=\"memoire\"):\n",
    "    n_layers = config[\"hidden\"]\n",
    "    width = config[\"width\"]\n",
    "    n_obs = config[\"n_obs\"]\n",
    "    decay = config[\"decay\"]\n",
    "    dataset = config[\"dataset\"]\n",
    "    lr = config[\"lr\"]\n",
    "    custom_layers = config[\"custom_layers\"]\n",
    "    lds = config[\"lds\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    classif_thresh = config[\"classif_thresh\"]\n",
    "    batch_norm = config[\"batch_norm\"]\n",
    "    patience = config[\"patience\"]\n",
    "    validation = config[\"validation\"]\n",
    "    optim_name = config[\"optim\"]\n",
    "    noval = validation is None\n",
    "\n",
    "    n_outputs = 1\n",
    "    pred_idx = 0\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    test_r2s = []\n",
    "    if exp_dir == None:\n",
    "        exp_dir == \"\"\n",
    "    l = []\n",
    "    for k, v in config.items():\n",
    "        l += [f\"{k}={v}\"]\n",
    "\n",
    "    exp_dir += \"/\" + \"/\".join(l)\n",
    "\n",
    "    # Train for 25 seeds\n",
    "    for i, seed in enumerate(seeds):\n",
    "        logdir = f\"runs/{exp_dir}/{seed=}\"\n",
    "        writer = SummaryWriter(log_dir=logdir)\n",
    "        min_val_loss = float(\"inf\")\n",
    "        min_train_loss = float(\"inf\")\n",
    "        val_activ_min_loss = None\n",
    "        train_activ_min_loss = None\n",
    "        test_activ_min_loss = None\n",
    "        val_activ_mintrain_loss = None\n",
    "        train_activ_mintrain_loss = None\n",
    "        test_activ_mintrain_loss = None\n",
    "\n",
    "        mintrain_epoch = 0\n",
    "        minval_epoch = 0\n",
    "\n",
    "        seed_train_losses = [np.nan] * n_epochs\n",
    "        seed_val_losses = [np.nan] * n_epochs\n",
    "        seed_test_losses = [np.nan] * n_epochs\n",
    "        seed_train_r2s = [np.nan] * n_epochs\n",
    "        seed_val_r2s = [np.nan] * n_epochs\n",
    "        seed_test_r2s = [np.nan] * n_epochs\n",
    "        early_stopping = EarlyStoppingActiv(patience=patience)\n",
    "\n",
    "        make_deterministic(seed=seed)\n",
    "\n",
    "        trainloader, training_data, X_val, y_val, n_dim, X_test, y_test = setup_data(\n",
    "            dataset,\n",
    "            batch_size,\n",
    "            n_obs,\n",
    "            lds,\n",
    "            classif_thresh,\n",
    "            validation,\n",
    "            dataset_path=\"/home/quo/Documents/Maitrise/optimneuralbandits/testing/datasets\",\n",
    "        )\n",
    "\n",
    "        X_train, y_train = training_data.combis, training_data.labels\n",
    "\n",
    "        if custom_layers is not None:\n",
    "            net = VariableNet(n_dim, custom_layers)\n",
    "        else:\n",
    "            net = Network(\n",
    "                n_dim, n_layers, n_outputs, width, dropout_rate, batch_norm\n",
    "            ).to(device)\n",
    "\n",
    "        if decay == \"epoch\":\n",
    "            decay_val = 1\n",
    "        else:\n",
    "            decay_val = decay\n",
    "\n",
    "        if lr == \"plateau\":\n",
    "            if optim_name == \"adam\":\n",
    "                optim = torch.optim.Adam(\n",
    "                    net.parameters(), lr=0.01, weight_decay=decay_val\n",
    "                )\n",
    "            else:\n",
    "                optim = torch.optim.SGD(\n",
    "                    net.parameters(), lr=0.01, weight_decay=decay_val\n",
    "                )\n",
    "            sched = ReduceLROnPlateau(optim, \"min\", patience=patience // 2)\n",
    "        else:\n",
    "            if optim_name == \"adam\":\n",
    "                optim = torch.optim.Adam(\n",
    "                    net.parameters(), lr=float(lr), weight_decay=decay_val\n",
    "                )\n",
    "            else:\n",
    "                optim = torch.optim.SGD(net.parameters(), lr=float(lr), weight_decay=decay_val)\n",
    "\n",
    "        ### RECORD MODEL ###\n",
    "        writer.add_graph(net, X_train)\n",
    "\n",
    "        for e in range(n_epochs):\n",
    "            if decay == \"epoch\":\n",
    "                optim.param_groups[0][\"weight_decay\"] = 1 / (e + 1)\n",
    "\n",
    "            ### TRAIN ###\n",
    "            for X, y in trainloader:\n",
    "                optim.zero_grad()\n",
    "                train_activ = net(X)\n",
    "                train_loss = criterion(train_activ, y)\n",
    "                train_loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            ### EVAL ###\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                # Compute losses\n",
    "                (\n",
    "                    train_activ,\n",
    "                    train_loss,\n",
    "                    val_activ,\n",
    "                    val_loss,\n",
    "                    test_activ,\n",
    "                    test_loss,\n",
    "                ) = get_losses_and_activ(\n",
    "                    net,\n",
    "                    criterion,\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_val,\n",
    "                    y_val,\n",
    "                    X_test,\n",
    "                    y_test,\n",
    "                )\n",
    "                net.train()\n",
    "\n",
    "                # Get R2 metric\n",
    "                train_r2 = r2_score(\n",
    "                    y_train.cpu().numpy(), train_activ.cpu().numpy()[:, pred_idx]\n",
    "                )\n",
    "                val_r2 = r2_score(\n",
    "                    y_val.cpu().numpy(), val_activ.cpu().numpy()[:, pred_idx]\n",
    "                )\n",
    "\n",
    "                # Save\n",
    "                seed_train_losses[e] = train_loss\n",
    "                seed_val_losses[e] = val_loss\n",
    "                seed_train_r2s[e] = train_r2\n",
    "                seed_val_r2s[e] = val_r2\n",
    "\n",
    "                writer.add_scalar(\"Loss/train\", train_loss, e)\n",
    "                writer.add_scalar(\"Loss/val\", val_loss, e)\n",
    "                writer.add_scalar(\"R2/train\", train_r2, e)\n",
    "                writer.add_scalar(\"R2/val\", val_r2, e)\n",
    "\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    test_r2 = r2_score(\n",
    "                        y_test.cpu().numpy(), test_activ.cpu().numpy()[:, pred_idx]\n",
    "                    )\n",
    "                    seed_test_losses[e] = test_loss\n",
    "                    seed_test_r2s[e] = test_r2\n",
    "                    writer.add_scalar(\"Loss/test\", test_loss, e)\n",
    "                    writer.add_scalar(\"R2/test\", test_r2, e)\n",
    "\n",
    "                # Update LR scheduler\n",
    "                if lr == \"plateau\":\n",
    "                    sched.step(val_loss)\n",
    "\n",
    "                (\n",
    "                    train_activ_min_loss,\n",
    "                    val_activ_min_loss,\n",
    "                    test_activ_min_loss,\n",
    "                    min_val_loss,\n",
    "                    train_activ_mintrain_loss,\n",
    "                    val_activ_mintrain_loss,\n",
    "                    test_activ_mintrain_loss,\n",
    "                    min_train_loss,\n",
    "                    mintrain_epoch,\n",
    "                    minval_epoch,\n",
    "                ) = update_minimums(\n",
    "                    train_loss,\n",
    "                    min_train_loss,\n",
    "                    train_activ,\n",
    "                    val_loss,\n",
    "                    min_val_loss,\n",
    "                    val_activ,\n",
    "                    test_loss,\n",
    "                    test_activ,\n",
    "                    pred_idx,\n",
    "                    val_activ_min_loss,\n",
    "                    train_activ_min_loss,\n",
    "                    test_activ_min_loss,\n",
    "                    val_activ_mintrain_loss,\n",
    "                    train_activ_mintrain_loss,\n",
    "                    test_activ_mintrain_loss,\n",
    "                    e,\n",
    "                    mintrain_epoch,\n",
    "                    minval_epoch,\n",
    "                )\n",
    "            ### VERIFY EARLY STOP ###\n",
    "            # Is weird rn but basically we just want to record the first early stop activations, but since we also want the lowest validation error's activation we can't break out yet\n",
    "            if not early_stopping.early_stop:\n",
    "                early_stopping(val_loss, train_activ, val_activ, test_activ)\n",
    "                # early_stopping(train_loss, train_activ, val_activ, test_activ)\n",
    "                if early_stopping.early_stop:\n",
    "                    ### PLOT EARLY STOP REPRESENTATION ###\n",
    "                    fig_pgt_es = plot_pred_vs_gt(\n",
    "                        y_train,\n",
    "                        early_stopping.train_activ,\n",
    "                        y_val,\n",
    "                        early_stopping.val_activ,\n",
    "                        y_test,\n",
    "                        early_stopping.test_activ,\n",
    "                        pred_idx=pred_idx,\n",
    "                        noval=noval,\n",
    "                    )\n",
    "                    writer.add_figure(\"pred_vs_gt_final\", fig_pgt_es)\n",
    "\n",
    "                    fig_pgt_es = plot_pred_vs_gt(\n",
    "                        y_train,\n",
    "                        early_stopping.train_activ,\n",
    "                        y_val,\n",
    "                        early_stopping.val_activ,\n",
    "                        y_test,\n",
    "                        early_stopping.test_activ,\n",
    "                        pred_idx=pred_idx,\n",
    "                        invert=True,\n",
    "                        noval=noval,\n",
    "                    )\n",
    "                    writer.add_figure(\"zinvert_pred_vs_gt_final\", fig_pgt_es)\n",
    "                    writer.flush()\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min val loss) ###\n",
    "        fig_pgt_minval = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_min_loss,\n",
    "            y_val,\n",
    "            val_activ_min_loss,\n",
    "            y_test,\n",
    "            test_activ_min_loss,\n",
    "            pred_idx=pred_idx,\n",
    "            noval=noval,\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"pred_vs_gt_minval\", fig_pgt_minval)\n",
    "\n",
    "        fig_pgt_minval = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_min_loss,\n",
    "            y_val,\n",
    "            val_activ_min_loss,\n",
    "            y_test,\n",
    "            test_activ_min_loss,\n",
    "            pred_idx=pred_idx,\n",
    "            invert=True,\n",
    "            noval=noval,\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"zinvert_pred_vs_gt_minval\", fig_pgt_minval)\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min train loss) ###\n",
    "        fig_pgt_mintrain = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_mintrain_loss,\n",
    "            y_val,\n",
    "            val_activ_mintrain_loss,\n",
    "            y_test,\n",
    "            test_activ_mintrain_loss,\n",
    "            pred_idx=pred_idx,\n",
    "            noval=noval,\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"pred_vs_gt_mintrain\", fig_pgt_mintrain)\n",
    "\n",
    "        fig_pgt_mintrain = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_mintrain_loss,\n",
    "            y_val,\n",
    "            val_activ_mintrain_loss,\n",
    "            y_test,\n",
    "            test_activ_mintrain_loss,\n",
    "            pred_idx=pred_idx,\n",
    "            invert=True,\n",
    "            noval=noval,\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"zinvert_pred_vs_gt_mintrain\", fig_pgt_mintrain)\n",
    "\n",
    "        ###\n",
    "\n",
    "        if not early_stopping.early_stop:\n",
    "            train_activ = train_activ.cpu().numpy()\n",
    "            val_activ = val_activ.cpu().numpy()\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_activ = test_activ.cpu().numpy()\n",
    "\n",
    "            fig_pgt_final = plot_pred_vs_gt(\n",
    "                y_train,\n",
    "                train_activ,\n",
    "                y_val,\n",
    "                val_activ,\n",
    "                y_test,\n",
    "                test_activ,\n",
    "                pred_idx=pred_idx,\n",
    "                noval=noval,\n",
    "            )\n",
    "\n",
    "            writer.add_figure(\"pred_vs_gt_final\", fig_pgt_final)\n",
    "\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        train_losses.append(seed_train_losses)\n",
    "        val_losses.append(seed_val_losses)\n",
    "        test_losses.append(seed_test_losses)\n",
    "        train_r2s.append(seed_train_r2s)\n",
    "        val_r2s.append(seed_val_r2s)\n",
    "        test_r2s.append(seed_test_r2s)\n",
    "    ### PLOT AGGREGATE DATA FOR ALL SEEDS ###\n",
    "    # logdir = f\"runs/{exp_dir}/aggregate\"\n",
    "    # writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    # ### PLOTS ###\n",
    "    # train_losses = np.array(train_losses)\n",
    "    # val_losses = np.array(val_losses)\n",
    "    # test_losses = np.array(test_losses)\n",
    "    # fig_loss =  plot_metric(n_epochs, train_losses, val_losses, test_losses)\n",
    "\n",
    "    # train_r2s = np.array(train_r2s)\n",
    "    # val_r2s = np.array(val_r2s)\n",
    "    # fig_r2 = plot_metric(n_epochs, train_r2s, val_r2s, test_r2s)\n",
    "\n",
    "    # writer.add_figure(\"losses\", fig_loss)\n",
    "    # writer.add_figure(\"r2s\", fig_r2)\n",
    "\n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    save_metrics(train_losses, f\"metrics/{exp_dir}/train_losses\")\n",
    "    save_metrics(val_losses, f\"metrics/{exp_dir}/val_losses\")\n",
    "    save_metrics(test_losses, f\"metrics/{exp_dir}/test_losses\")\n",
    "    save_metrics(train_r2s, f\"metrics/{exp_dir}/train_r2s\")\n",
    "    save_metrics(val_r2s, f\"metrics/{exp_dir}/val_r2s\")\n",
    "    save_metrics(test_r2s, f\"metrics/{exp_dir}/test_r2s\")\n",
    "\n",
    "    # print(f\"saved at runs/{exp_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "for config in deepcopy(configs):\n",
    "    # config[\"modif\"] = \"tune_res\"\n",
    "    run_config(config)\n",
    "\n",
    "# for config in deepcopy(configs):\n",
    "#     config[\"batch_norm\"] = True\n",
    "#     config[\"modif\"] = \"with_bn\"\n",
    "#     run_config(config)\n",
    "\n",
    "# for config in deepcopy(configs):\n",
    "#     config[\"decay\"] = \"epoch\"\n",
    "#     config[\"modif\"] = \"epoch_decay\"\n",
    "#     run_config(config)\n",
    "\n",
    "# for config in deepcopy(configs):\n",
    "#     config[\"decay\"] = \"epoch\"\n",
    "#     config[\"batch_norm\"] = True\n",
    "#     config[\"modif\"] = \"epoch_decay_bn\"\n",
    "#     run_config(config)\n",
    "\n",
    "# for config in deepcopy(configs):\n",
    "#     config[\"modif\"] = \"tune_res\"\n",
    "#     config[\"patience\"] = 25\n",
    "#     run_config(config)\n",
    "\n",
    "# for config in deepcopy(configs):\n",
    "#     run_config(config)\n",
    "    \n",
    "\n",
    "# with Pool(4) as p:\n",
    "#     p.map(run_config, conf1igs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarque sur le weight decay\n",
    "\n",
    "Si on passe par le weight decay avec les epoques et que l'on utilise aucun set de validation. Alors LDS = True semble mieux que sqrt_inv comme ca force davantage le modele a se concentrer sur les observations a faible frequence. Learning rate en plateau semble egalement mieux. Une batch size plus petite semble egalement meilleure en terme de representation finale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations 3 aout 2022\n",
    "* 1 layer, 128 est suffisant pour overfit 99999 observations. Super.\n",
    "* On veut utiliser le decay dans notre cas pour regulariser le reseau tot dans son apprentissage pour eviter d'overfit juste le set d'entrainement (moins il a d'observations, plus il devrait etre regu dans un bandit pcq plus le set d'entrainement est petit, plus il risque d'overfit le set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': '500_rx_100000_combis_10_patterns_23',\n",
       "  'width': 128,\n",
       "  'hidden': 1,\n",
       "  'n_obs': 20000,\n",
       "  'decay': 0,\n",
       "  'lr': 'plateau',\n",
       "  'custom_layers': None,\n",
       "  'lds': True,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': None,\n",
       "  'loss': ['mse'],\n",
       "  'classif_thresh': None,\n",
       "  'batch_norm': False,\n",
       "  'patience': 50,\n",
       "  'validation': None,\n",
       "  'optim': 'adam',\n",
       "  'modif': 'lds_nodecay'},\n",
       " {'dataset': '500_rx_100000_combis_10_patterns_23',\n",
       "  'width': 128,\n",
       "  'hidden': 1,\n",
       "  'n_obs': 20000,\n",
       "  'decay': 0,\n",
       "  'lr': 'plateau',\n",
       "  'custom_layers': None,\n",
       "  'lds': None,\n",
       "  'batch_size': 128,\n",
       "  'dropout_rate': None,\n",
       "  'loss': ['mse'],\n",
       "  'classif_thresh': None,\n",
       "  'batch_norm': False,\n",
       "  'patience': 50,\n",
       "  'validation': None,\n",
       "  'optim': 'adam',\n",
       "  'modif': 'lds_nodecay'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
