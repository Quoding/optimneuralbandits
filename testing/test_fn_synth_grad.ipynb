{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import normalize\n",
    "from math import sqrt\n",
    "from detorch import DE, Policy, Strategy\n",
    "from detorch.config import default_config, Config\n",
    "from typing import Type\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('..')\n",
    "sys.path.append('viz')\n",
    "from optimneuralts import OptimNeuralTS, LenientOptimNeuralTS\n",
    "from networks import NetworkDropout\n",
    "from utils import do_gradient_optim\n",
    "import viz_config\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config(default_config)\n",
    "bounds = [-2.5, 1.5]\n",
    "theta = torch.Tensor([0, 3, -2, -4, 1, 1]).to(device)\n",
    "d = 1\n",
    "torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PullPolicy(Policy):\n",
    "    def __init__(self, eval_fn):\n",
    "        super().__init__()\n",
    "        self.point = torch.FloatTensor(1).uniform_(*bounds).to(device)\n",
    "        self.params = nn.Parameter(\n",
    "            self.point, requires_grad=False\n",
    "        )\n",
    "        self.eval_fn = eval_fn\n",
    "        self.ucb = None\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.transform()\n",
    "        ucb, activation_grad, _, _ = self.eval_fn(self.point)\n",
    "        ucb = ucb.detach().item()\n",
    "        self.activation_grad = activation_grad\n",
    "        self.ucb = ucb\n",
    "        # logging.info(self.point)\n",
    "        # logging.info(ucb)\n",
    "        return ucb\n",
    "\n",
    "    def transform(self):\n",
    "        self.point = torch.clip(self.params, *bounds).to(device)\n",
    "        self.params = nn.Parameter(\n",
    "            self.point, requires_grad=False\n",
    "        )\n",
    "        # return generate_feature_vector_from_point(self.point)\n",
    "\n",
    "\n",
    "class DEConfig:\n",
    "    n_step: int = 3\n",
    "    population_size: int = 60\n",
    "    differential_weight: float = 0.8\n",
    "    crossover_probability: float = 0.9\n",
    "    strategy: Strategy = Strategy.best1bin\n",
    "    seed: int = \"does not matter\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(found_solution: set, true_solution: set):\n",
    "    n_in_inter = 0\n",
    "\n",
    "    intersection = found_solution & true_solution\n",
    "\n",
    "    n_in_inter = len(intersection)\n",
    "\n",
    "    return (\n",
    "        n_in_inter / (len(found_solution) + len(true_solution) - n_in_inter),\n",
    "        n_in_inter,\n",
    "    )\n",
    "def make_deterministic(seed):\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Built-in Python\n",
    "    random.seed(seed)\n",
    "\n",
    "def project_point(point):\n",
    "    return torch.tensor([1, point, point ** 2, point ** 3, point ** 4, point ** 5]).to(device)\n",
    "\n",
    "def reward_fn(point, add_noise=True):\n",
    "    noise = torch.normal(0, 1, (1,)).item()\n",
    "\n",
    "    vec = project_point(point)\n",
    "    value = theta @ vec.T\n",
    "    \n",
    "    return value + add_noise * noise\n",
    "\n",
    "\n",
    "def gen_warmup_vecs_and_rewards(n_warmup):\n",
    "    vecs = torch.tensor([])\n",
    "    rewards = torch.tensor([])\n",
    "    for i in range(n_warmup):\n",
    "        point = torch.FloatTensor(1).uniform_(bounds[0], bounds[1]).to(device)\n",
    "        reward = torch.tensor([reward_fn(point)])\n",
    "\n",
    "        vecs = torch.cat((vecs, point))\n",
    "        rewards = torch.cat((rewards, reward))\n",
    "    vecs = vecs.view((n_warmup, -1))\n",
    "    rewards = rewards.view((n_warmup, -1))\n",
    "    return vecs, rewards\n",
    "\n",
    "\n",
    "def plot_estimate(agent, trial, fn=None, title=\"\"):\n",
    "    n_points = 1000\n",
    "    x = np.linspace(-2.5, 1.5, n_points)\n",
    "    x_vec = []\n",
    "    y = []\n",
    "    for x_point in x:\n",
    "        y.append(reward_fn(float(x_point), add_noise=False).cpu().numpy())\n",
    "\n",
    "    y_pred = []\n",
    "    cbs = []\n",
    "    ucbs = []\n",
    "    x_tns = torch.from_numpy(x)\n",
    "    x_tns = x_tns.view(n_points, 1).float()\n",
    "\n",
    "    for point in x_tns:\n",
    "        sample, _, activ, cb = agent.get_sample(point.to(device))\n",
    "        y_pred.append(activ)\n",
    "        cbs.append(3 * cb)\n",
    "        ucbs.append(sample)\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    cbs = np.array(cbs)\n",
    "\n",
    "    point_played = agent.train_dataset.features.squeeze(0).cpu().numpy()\n",
    "    rewards_rec = agent.train_dataset.rewards.squeeze(0).cpu().numpy()\n",
    "    n_played = point_played.shape[0]\n",
    "\n",
    "    # plt.ylim(-20, 10)\n",
    "    # plt.ylim(-25, 10)\n",
    "    plt.plot(x, y, color=\"tab:blue\", label=\"Vraie fonction\")\n",
    "    plt.plot(x, y_pred, color=\"tab:orange\", label=\"Estimation de la fonction\")\n",
    "    # plt.fill_between(x, y_pred, ucbs, color='tab:red', alpha=0.3)\n",
    "    plt.fill_between(\n",
    "        x, y_pred + cbs, y_pred - cbs, alpha=0.3, color=\"tab:orange\", zorder=-1, label=\"Intervalle de confiance\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        x,\n",
    "        [0] * n_points,\n",
    "        color=\"black\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=\"Seuil bonne/mauvaise action\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        point_played[:n_played],\n",
    "        rewards_rec[:n_played],\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        label=\"Points joués précédemment\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        point_played[-1], rewards_rec[-1], color=\"green\", label=\"Dernier point joué\"\n",
    "    )\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    if fn is None:\n",
    "        filename = f\"viz/images/exp_poly/regTS_{n_trials}_trials_expl_{exploration_mult}_trial_{trial}.png\"\n",
    "    else:\n",
    "        filename = f\"viz/images/exp_poly/{fn}.png\"\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def find_best_member(eval_fn, de_config, seed):\n",
    "    de_config.seed = seed\n",
    "    config = Config(default_config)\n",
    "\n",
    "    @config(\"policy\")\n",
    "    class PolicyConfig:\n",
    "        policy: Type[Policy] = PullPolicy\n",
    "        eval_fn: object = agent.get_sample\n",
    "\n",
    "    config(\"de\")(de_config)\n",
    "\n",
    "    de = DE(config)\n",
    "    de.train()\n",
    "\n",
    "    return de.population[de.current_best]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize 1 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 42\n",
    "make_deterministic(run)\n",
    "\n",
    "n_trials = 500\n",
    "dropout_rate = False\n",
    "width = 100\n",
    "net = NetworkDropout(d, width,dropout=dropout_rate).to(device)\n",
    "reg = 1\n",
    "exploration_mult = 1\n",
    "delay = 0\n",
    "reward_fn = reward_fn\n",
    "style=\"ts\"\n",
    "sampletype=\"r\"\n",
    "max_n_steps = 100 \n",
    "agent = OptimNeuralTS(net, nu=exploration_mult, lambda_=reg, style=style, sampletype=sampletype)\n",
    "# agent = LenientOptimNeuralTS(net, nu=exploration_mult, lambda_=reg, reward_sample_thresholds=[float('-inf'), 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "\n",
    "example_vec = vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train_dataset.set_(vecs, rewards)\n",
    "\n",
    "for vec in vecs:\n",
    "    activ, grad = agent.compute_activation_and_grad(vec)\n",
    "    agent.U += grad * grad\n",
    "\n",
    "agent.net.eval()\n",
    "agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "agent.net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_estimate(agent, 0,  f'grad_{style}_expl_{exploration_mult}_{sampletype}_run{run}_trial_0')\n",
    "agent.net.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 1 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_trials):\n",
    "    agent.net.train()\n",
    "    a_t, idx, best_member_grad = do_gradient_optim(\n",
    "        agent, 3 * 60, x, lr=1e-2\n",
    "    )\n",
    "    agent.net.eval()\n",
    "\n",
    "    r_t = reward_fn(a_t).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    logging.info(f\"trial: {i}\")\n",
    "\n",
    "    agent.U += best_member_grad * best_member_grad\n",
    "\n",
    "    agent.train_dataset.add(a_t, r_t)\n",
    "    agent.net.train()\n",
    "    agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "    agent.net.eval()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        plot_estimate(\n",
    "            agent,\n",
    "            0,\n",
    "            f\"grad_{style}_expl_{exploration_mult}_{sampletype}_run{run}_trial_{i+1}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all runs, for all algos and every exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.linspace(-2.5, 1.5, 1000)\n",
    "x_arr = x_arr.reshape(1000, 1)\n",
    "x = torch.from_numpy(x_arr).to(device).float()\n",
    "y = []\n",
    "for point in x:\n",
    "    y.append(reward_fn(point, add_noise=False).cpu().numpy())\n",
    "\n",
    "y = np.array(y)\n",
    "true_sol_idx = np.where(y >= 0)[0]\n",
    "true_sol = x[true_sol_idx].to(device).float()\n",
    "true_sol_idx = set(true_sol_idx.tolist())\n",
    "\n",
    "n_true_sol = len(true_sol)\n",
    "logging.info(n_true_sol)\n",
    "n_sigmas = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "algos = [\"UCB\", \"regTS\"]\n",
    "exploration_mults = [1, 10]\n",
    "logging.info(metrics_dict)\n",
    "max_n_steps = 10\n",
    "for algo in algos:\n",
    "    metrics_dict[algo] = {}\n",
    "    for exploration_mult in exploration_mults:\n",
    "        metrics_dict[algo][str(exploration_mult)] = {}\n",
    "        metrics_dict[algo][str(exploration_mult)][\"jaccards\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"percent_inter\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"percent_found\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"fails\"] = 0\n",
    "\n",
    "        b = 0\n",
    "\n",
    "        for i in range(b, 50):\n",
    "            logging.info(f\"at algo: {algo}, expl_mult: {exploration_mult}, run: {i}\")\n",
    "            make_deterministic(seed=i)\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            n_trials = 500\n",
    "            width = 100\n",
    "            net = NetworkDropout(d, width).to(device)\n",
    "            reg = 1\n",
    "            delay = 0\n",
    "            reward_fn = reward_fn\n",
    "            de_config = DEConfig\n",
    "            de_policy = PullPolicy\n",
    "            lr = 1e-2\n",
    "\n",
    "            if algo == \"UCB\":\n",
    "                agent = OptimNeuralTS(\n",
    "                    net, nu=exploration_mult, lambda_=reg, style=\"ucb\"\n",
    "                )\n",
    "            elif algo == \"regTS\":\n",
    "                agent = OptimNeuralTS(\n",
    "                    net, nu=exploration_mult, lambda_=reg, style=\"ts\"\n",
    "                )\n",
    "            elif algo == \"lenientTS\":\n",
    "                agent = LenientOptimNeuralTS(\n",
    "                    [float(\"-inf\"), 0],\n",
    "                    net=net,\n",
    "                    nu=exploration_mult,\n",
    "                    lambda_=reg,\n",
    "                )\n",
    "\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            # Warmup\n",
    "            for vec in vecs:\n",
    "                activ, grad = agent.compute_activation_and_grad(vec)\n",
    "                agent.U += grad * grad\n",
    "\n",
    "            agent.train_dataset.set_(vecs, rewards)\n",
    "            agent.net.train()\n",
    "            agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "            agent.net.eval()\n",
    "\n",
    "            # Playing\n",
    "            for j in range(n_trials):\n",
    "                a_t, idx, best_member_grad = do_gradient_optim(agent, 3 * 60, x, lr=1e-2)\n",
    "                r_t = reward_fn(a_t).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                agent.U += best_member_grad * best_member_grad\n",
    "\n",
    "                agent.train_dataset.add(a_t, r_t)\n",
    "\n",
    "                agent.net.train()\n",
    "                agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "                agent.net.eval()   \n",
    "\n",
    "            sol, _, _ = agent.find_solution_in_vecs(x, 0, n_sigmas=n_sigmas)\n",
    "            # sol = sol.to(device)\n",
    "            n_sol = len(sol)\n",
    "\n",
    "            jaccard, n_inter = compute_jaccard(sol, true_sol_idx)\n",
    "            percent_found = n_inter / n_true_sol\n",
    "            if n_sol == 0:\n",
    "                percent_inter = 0\n",
    "            else:\n",
    "                percent_inter = n_inter / n_sol\n",
    "\n",
    "            metrics_dict[algo][str(exploration_mult)][\"jaccards\"].append(jaccard)\n",
    "            metrics_dict[algo][str(exploration_mult)][\"percent_inter\"].append(\n",
    "                percent_inter\n",
    "            )\n",
    "            metrics_dict[algo][str(exploration_mult)][\"percent_found\"].append(\n",
    "                percent_found\n",
    "            )\n",
    "\n",
    "            if n_sol == 0:\n",
    "                logging.info(f\"Found no solution for run {i}\")\n",
    "                metrics_dict[algo][str(exploration_mult)][\"fails\"] += 1\n",
    "\n",
    "                plot_estimate(\n",
    "                    agent,\n",
    "                    n_trials,\n",
    "                    fn=f\"grad_no_sol_{algo}_expl_{exploration_mult}_100_trials_seed_{i}\",\n",
    "                )\n",
    "\n",
    "            logging.info(\n",
    "                f\"jaccard: {jaccard}, percent_inter: {percent_inter}, percent_found: {percent_found}\"\n",
    "            )\n",
    "\n",
    "torch.save(metrics_dict, f\"metrics_dict_exp_synth_{max_n_steps}_basic.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"GRAD_REGULAR\")\n",
    "logging.info(\"GRAD_REGULAR\")\n",
    "logging.info(\"GRAD_REGULAR\")\n",
    "for algo in metrics_dict.keys():\n",
    "    logging.info(f\"Algo: {algo}\")\n",
    "    for expl_mult in metrics_dict[algo].keys():\n",
    "        logging.info(f'Expl mult: {expl_mult}')\n",
    "        for metric in  metrics_dict[algo][expl_mult].keys():\n",
    "            logging.info(f'Metric: {metric}')\n",
    "            logging.info(f\"mean: {np.mean(metrics_dict[algo][expl_mult][metric])} +- {np.std(metrics_dict[algo][expl_mult][metric])} \")\n",
    "            logging.info(f\"interval:  [{np.min(metrics_dict[algo][expl_mult][metric])}, {np.max(metrics_dict[algo][expl_mult][metric])}]\")\n",
    "            logging.info(\"============================================\")\n",
    "        # if 0 in metrics_dict[algo][expl_mult]:\n",
    "        #     logging.info(f'rerun {metrics_dict[algo][expl_mult][metric].index(0)} with plotting')\n",
    "    \n",
    "    logging.info(\"============================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test avec weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "# algos = [\"UCB\", \"regTS\", \"lenientTS\"]\n",
    "algos = [\"UCB\", \"regTS\"]\n",
    "exploration_mults = [1, 10]\n",
    "logging.info(metrics_dict)\n",
    "max_n_steps = 10\n",
    "for algo in algos:\n",
    "    metrics_dict[algo] = {}\n",
    "    for exploration_mult in exploration_mults:\n",
    "        metrics_dict[algo][str(exploration_mult)] = {}\n",
    "        metrics_dict[algo][str(exploration_mult)][\"jaccards\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"percent_inter\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"percent_found\"] = []\n",
    "        metrics_dict[algo][str(exploration_mult)][\"fails\"] = 0\n",
    "\n",
    "        b = 0\n",
    "\n",
    "        for i in range(b, 50):\n",
    "            logging.info(f\"at algo: {algo}, expl_mult: {exploration_mult}, run: {i}\")\n",
    "            make_deterministic(seed=i)\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            n_trials = 500\n",
    "            width = 100\n",
    "            net = NetworkDropout(d, width).to(device)\n",
    "            reg = 1\n",
    "            reward_fn = reward_fn\n",
    "            de_config = DEConfig\n",
    "            de_policy = PullPolicy\n",
    "            lr = 1e-2\n",
    "\n",
    "            if algo == \"UCB\":\n",
    "                agent = OptimNeuralTS(\n",
    "                    net, nu=exploration_mult, lambda_=reg, style=\"ucb\"\n",
    "                )\n",
    "            elif algo == \"regTS\":\n",
    "                agent = OptimNeuralTS(net, nu=exploration_mult, lambda_=reg, style=\"ts\")\n",
    "            elif algo == \"lenientTS\":\n",
    "                agent = LenientOptimNeuralTS(\n",
    "                    [float(\"-inf\"), 0],\n",
    "                    net=net,\n",
    "                    nu=exploration_mult,\n",
    "                    lambda_=reg,\n",
    "                )\n",
    "\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            agent.train_dataset.set_(vecs, rewards)\n",
    "\n",
    "            # Warmup\n",
    "            for vec in vecs:\n",
    "                activ, grad = agent.compute_activation_and_grad(vec)\n",
    "                agent.U += grad * grad\n",
    "\n",
    "            agent.net.train()\n",
    "            agent.train(max_n_steps, patience=max_n_steps, lds=False, use_decay=True)\n",
    "            agent.net.eval()\n",
    "\n",
    "            # Playing\n",
    "            for j in range(n_trials):\n",
    "                agent.net.train()\n",
    "                a_t, idx, best_member_grad = do_gradient_optim(agent, 60, x, lr=1e-2)\n",
    "                agent.net.eval()\n",
    "                r_t = reward_fn(a_t).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                agent.U += best_member_grad * best_member_grad\n",
    "\n",
    "                agent.train_dataset.add(a_t, r_t)\n",
    "\n",
    "                agent.net.train()\n",
    "                agent.train(max_n_steps, patience=max_n_steps, lds=False, use_decay=True)\n",
    "                agent.net.eval()\n",
    "\n",
    "            # Stop using mask in evaluation\n",
    "            sol, _, _ = agent.find_solution_in_vecs(x, 0, n_sigmas=n_sigmas)\n",
    "            # sol = sol.to(device)\n",
    "            n_sol = len(sol)\n",
    "\n",
    "            jaccard, n_inter = compute_jaccard(sol, true_sol_idx)\n",
    "            percent_found = n_inter / n_true_sol\n",
    "            if n_sol == 0:\n",
    "                percent_inter = 0\n",
    "            else:\n",
    "                percent_inter = n_inter / n_sol\n",
    "\n",
    "            metrics_dict[algo][str(exploration_mult)][\"jaccards\"].append(jaccard)\n",
    "            metrics_dict[algo][str(exploration_mult)][\"percent_inter\"].append(\n",
    "                percent_inter\n",
    "            )\n",
    "            metrics_dict[algo][str(exploration_mult)][\"percent_found\"].append(\n",
    "                percent_found\n",
    "            )\n",
    "\n",
    "            if n_sol == 0:\n",
    "                logging.info(f\"Found no solution for run {i}\")\n",
    "                plot_estimate(\n",
    "                    agent,\n",
    "                    n_trials,\n",
    "                    fn=f\"grad_no_sol_{algo}_decay_expl_{exploration_mult}_100_trials_seed_{i}\",\n",
    "                )\n",
    "                metrics_dict[algo][str(exploration_mult)][\"fails\"] += 1\n",
    "\n",
    "\n",
    "            logging.info(\n",
    "                f\"jaccard: {jaccard}, percent_inter: {percent_inter}, percent_found: {percent_found}\"\n",
    "            )\n",
    "\n",
    "torch.save(metrics_dict, f\"metrics_dict_exp_synth_{max_n_steps}_decay.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"GRAD_L2\")\n",
    "logging.info(\"GRAD_L2\")\n",
    "logging.info(\"GRAD_L2\")\n",
    "for algo in metrics_dict.keys():\n",
    "    logging.info(f\"Algo: {algo}\")\n",
    "    for expl_mult in metrics_dict[algo].keys():\n",
    "        logging.info(f'Expl mult: {expl_mult}')\n",
    "        for metric in  metrics_dict[algo][expl_mult].keys():\n",
    "            logging.info(f'Metric: {metric}')\n",
    "            logging.info(f\"mean: {np.mean(metrics_dict[algo][expl_mult][metric])} +- {np.std(metrics_dict[algo][expl_mult][metric])} \")\n",
    "            logging.info(f\"interval:  [{np.min(metrics_dict[algo][expl_mult][metric])}, {np.max(metrics_dict[algo][expl_mult][metric])}]\")\n",
    "            logging.info(\"============================================\")\n",
    "        # if 0 in metrics_dict[algo][expl_mult]:\n",
    "        #     logging.info(f'rerun {metrics_dict[algo][expl_mult][metric].index(0)} with plotting')\n",
    "    \n",
    "    logging.info(\"============================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep dropout active all the time instead of turning it off during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "algos = [\"regTS\"]\n",
    "logging.info(metrics_dict)\n",
    "max_n_steps = 10\n",
    "for algo in algos:\n",
    "    metrics_dict[algo] = {}\n",
    "    for dropout_rate in [0.2, 0.5, 0.8]:\n",
    "        metrics_dict[algo][str(dropout_rate)] = {}\n",
    "        metrics_dict[algo][str(dropout_rate)][\"jaccards\"] = []\n",
    "        metrics_dict[algo][str(dropout_rate)][\"percent_inter\"] = []\n",
    "        metrics_dict[algo][str(dropout_rate)][\"percent_found\"] = []\n",
    "        metrics_dict[algo][str(dropout_rate)][\"fails\"] = 0\n",
    "        b = 0\n",
    "\n",
    "        for i in range(b, 50):\n",
    "            logging.info(f\"at algo: {algo}, dropout: {dropout_rate}, run: {i}\")\n",
    "            make_deterministic(seed=i)\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            n_trials = 500\n",
    "            width = 100\n",
    "            net = NetworkDropout(d, width, dropout=dropout_rate).to(device)\n",
    "            reg = 1\n",
    "            sampletype = \"f\"\n",
    "            reward_fn = reward_fn\n",
    "            de_config = DEConfig\n",
    "            de_policy = PullPolicy\n",
    "            bern_p = 1 - dropout_rate\n",
    "            p_vec = torch.tensor([bern_p] * width)\n",
    "\n",
    "            agent = OptimNeuralTS(\n",
    "                net, lambda_=reg, style=\"ts\", sampletype=sampletype\n",
    "            )\n",
    "\n",
    "            vecs, rewards = gen_warmup_vecs_and_rewards(10)\n",
    "            vecs, rewards = vecs.to(device), rewards.to(device)\n",
    "            agent.train_dataset.set_(vecs, rewards)\n",
    "\n",
    "            # Warmup\n",
    "            for vec in vecs:\n",
    "                activ, grad = agent.compute_activation_and_grad(vec)\n",
    "                agent.U += grad * grad\n",
    "\n",
    "                # agent.net.eval()\n",
    "            agent.net.train()\n",
    "            agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "\n",
    "            # Train\n",
    "            for j in range(n_trials):\n",
    "                agent.net.train()\n",
    "\n",
    "                a_t, idx, best_member_grad = do_gradient_optim(agent, 60, x, lr=1e-2)\n",
    "                r_t = reward_fn(a_t).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                if best_member_grad is None:\n",
    "                    break\n",
    "                agent.U += best_member_grad * best_member_grad\n",
    "\n",
    "                agent.train_dataset.add(a_t, r_t)\n",
    "\n",
    "\n",
    "                # agent.net.eval()\n",
    "                agent.train(max_n_steps, patience=max_n_steps, lds=False)\n",
    "\n",
    "            if best_member_grad is None:\n",
    "                metrics_dict[algo][str(dropout_rate)][\"fails\"] += 1\n",
    "                logging.info(\n",
    "                    f\"Encountered a fail in {algo} {dropout_rate} because of nans\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            agent.net.train()\n",
    "            sol, _, _ = agent.find_solution_in_vecs(x, 0, n_sigmas=n_sigmas)\n",
    "            # sol = sol.to(device)\n",
    "            n_sol = len(sol)\n",
    "\n",
    "            jaccard, n_inter = compute_jaccard(sol, true_sol_idx)\n",
    "            percent_found = n_inter / n_true_sol\n",
    "            if n_sol == 0:\n",
    "                percent_inter = 0\n",
    "            else:\n",
    "                percent_inter = n_inter / n_sol\n",
    "\n",
    "            metrics_dict[algo][str(dropout_rate)][\"jaccards\"].append(jaccard)\n",
    "            metrics_dict[algo][str(dropout_rate)][\"percent_inter\"].append(percent_inter)\n",
    "            metrics_dict[algo][str(dropout_rate)][\"percent_found\"].append(percent_found)\n",
    "\n",
    "            if n_sol == 0:\n",
    "                logging.info(f\"Found no solution for run {i}\")\n",
    "                metrics_dict[algo][str(dropout_rate)][\"fails\"] += 1\n",
    "\n",
    "                plot_estimate(\n",
    "                    agent,\n",
    "                    n_trials,\n",
    "                    fn=f\"grad_no_sol_{algo}_drop_{dropout_rate}_100_trials_seed_{i}.png\",\n",
    "                )\n",
    "\n",
    "            logging.info(\n",
    "                f\"jaccard: {jaccard}, percent_inter: {percent_inter}, percent_found: {percent_found}\"\n",
    "            )\n",
    "\n",
    "torch.save(metrics_dict, f\"metrics_dict_exp_synth_{max_n_steps}_dropout.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"GRAD_DROPOUT\")\n",
    "logging.info(\"GRAD_DROPOUT\")\n",
    "logging.info(\"GRAD_DROPOUT\")\n",
    "for algo in metrics_dict.keys():\n",
    "    logging.info(f\"Algo: {algo}\")\n",
    "    for dropout_rate in metrics_dict[algo].keys():\n",
    "        logging.info(f'dropout: {dropout_rate}')\n",
    "        for metric in  metrics_dict[algo][dropout_rate].keys():\n",
    "            logging.info(f'Metric: {metric}')\n",
    "            logging.info(f\"mean: {np.mean(metrics_dict[algo][dropout_rate][metric])} +- {np.std(metrics_dict[algo][dropout_rate][metric])} \")\n",
    "            logging.info(f\"interval:  [{np.min(metrics_dict[algo][dropout_rate][metric])}, {np.max(metrics_dict[algo][dropout_rate][metric])}]\")\n",
    "            logging.info(\"============================================\")\n",
    "        # if 0 in metrics_dict[algo][expl_mult]:\n",
    "        #     logging.info(f'rerun {metrics_dict[algo][expl_mult][metric].index(0)} with plotting')\n",
    "    \n",
    "    logging.info(\"============================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
