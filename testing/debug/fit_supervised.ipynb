{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "from optimneuralts import DENeuralTSDiag, LenientDENeuralTSDiag\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from itertools import product\n",
    "import io\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # default = (6.4, 4.8)\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 140  # default = 100\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "title_font_size = \"10\"\n",
    "\n",
    "THRESH = 1.1\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, dim, n_hidden_layers, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(dim, hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            \n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        return self.layers[-1](x)\n",
    "\n",
    "class ReducingNet(nn.Module):\n",
    "    def __init__(self, dim, layer_widths):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(dim, layer_widths[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(len(layer_widths) - 1):\n",
    "            self.layers.append(nn.Linear(layer_widths[i], layer_widths[i + 1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(layer_widths[-1], 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        \n",
    "class CombiDataset(Dataset):\n",
    "    def __init__(self, combis, risks):\n",
    "        self.combis = combis\n",
    "        self.risks = risks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.risks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.combis[idx], self.risks[idx]\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.count = 0\n",
    "        self.train_activ = None\n",
    "        self.val_activ = None\n",
    "\n",
    "    def __call__(self, cur_loss, train_activ, val_activ):\n",
    "        # If no improvement\n",
    "        if cur_loss >= self.min_loss:\n",
    "            self.count += 1\n",
    "        else: # Improvement, store activs\n",
    "            self.count = 0\n",
    "            self.store(train_activ, val_activ)\n",
    "            self.min_loss = cur_loss\n",
    "\n",
    "    def store(self, train_activ, val_activ):\n",
    "        self.train_activ = train_activ.detach().clone()\n",
    "        self.val_activ = val_activ.detach().clone()\n",
    "\n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        if self.count >= self.patience:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path):\n",
    "    dataset = pd.read_csv(\"../datasets/combinations/\" + dataset_path + \".csv\")\n",
    "\n",
    "    with open(\"../datasets/patterns/\" + dataset_path + \".json\", \"r\") as f:\n",
    "        patterns = json.load(f)\n",
    "\n",
    "    features = dataset.iloc[:, :-3]\n",
    "    risks = dataset.iloc[:, -3]\n",
    "\n",
    "    n_obs, n_dim = features.shape\n",
    "\n",
    "    return features, risks, patterns, n_obs, n_dim\n",
    "\n",
    "def setup_data(dataset, batch_size, n_obs):\n",
    "    combis, risks, _, _, n_dim = load_dataset(dataset)\n",
    "\n",
    "    combis, risks = (\n",
    "        torch.tensor(combis.values).float(),\n",
    "        torch.tensor(risks.values).unsqueeze(1).float(),\n",
    "    )\n",
    "\n",
    "\n",
    "    X_train, y_train = combis[:n_obs], risks[:n_obs]\n",
    "    X_val, y_val = combis[n_obs:], risks[n_obs:]\n",
    "\n",
    "    training_data = CombiDataset(X_train, y_train)\n",
    "    trainloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "    return trainloader, X_train, y_train, X_val, y_val, n_dim\n",
    "\n",
    "\n",
    "def plot_losses(n_epochs, train_losses, val_losses):\n",
    "    fig = plt.figure()\n",
    "    x = list(range(n_epochs))\n",
    "    train_nanmean = np.nanmean(train_losses, axis=0)\n",
    "    train_nanstd = np.nanstd(train_losses, axis=0)\n",
    "    val_nanmean = np.nanmean(val_losses, axis=0)\n",
    "    val_nanstd = np.nanstd(val_losses, axis=0)\n",
    "\n",
    "    # plot training loss\n",
    "    plt.plot(x, train_nanmean, label=\"Entrainement\", color=\"tab:blue\")\n",
    "    plt.fill_between(x, train_nanmean - train_nanstd, train_nanmean + train_nanstd, alpha=0.3, color=\"tab:blue\")\n",
    "\n",
    "    # plot val loss\n",
    "    plt.plot(x, val_nanmean, label=\"Validation\", color=\"tab:orange\")\n",
    "    plt.fill_between(x, val_nanmean - val_nanstd, val_nanmean + val_nanstd, alpha=0.3, color=\"tab:orange\")\n",
    "\n",
    "    plt.title(\"Pertes selon le nombre d'époques\")\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"Perte\")\n",
    "    plt.ylim(0, 1.2)\n",
    "    plt.xlim(0, n_epochs)\n",
    "    plt.yticks(np.arange(0, 1 + 0.05, 0.05))\n",
    "    plt.xticks(np.arange(0, n_epochs, 1))\n",
    "    plt.legend()\n",
    "\n",
    "    return fig\n",
    "    \n",
    "def plot_pred_vs_gt(true, pred, title):\n",
    "    fig = plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Vérité\")\n",
    "    plt.ylabel(\"Prédiction\")\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlim(0, 3)\n",
    "    plt.scatter(true, pred, alpha=0.1)\n",
    "    plt.plot([0, 3], [0, 3], color=\"black\", linestyle=\"dashed\", label=\"Perfection\")\n",
    "    plt.legend()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def save_metrics(array, path):\n",
    "    dir_ = \"/\".join(path.split(\"/\")[:-1])\n",
    "    os.makedirs(dir_, exist_ok=True)\n",
    "    np.save(path, array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "param_values = {\"width\":[32, 128], \"hidden\": [1,2,3], \"n_obs\": [100, 1000, 10000], \"dataset\": [\"50_rx_100000_combis_4_patterns_3\", \"100_rx_100000_combis_10_patterns_35\", \"1000_rx_100000_combis_10_patterns_25\"], \"decay\": [0], \"lr\": [1e-2]}\n",
    "\n",
    "configs = [dict(zip(param_values, v)) for v in product(*param_values.values())]\n",
    "\n",
    "# configs = [{\"width\": 32, \"n_obs\": 1000, \"dataset\":\"50_rx_100000_combis_4_patterns_3\", \"decay\": 0, \"hidden\": 3, \"lr\": 1e-2}, {\"width\": 32, \"n_obs\": 100, \"dataset\":\"50_rx_100000_combis_4_patterns_3\", \"decay\": 0, \"hidden\": 3, \"lr\": 1e-2}]\n",
    "\n",
    "print(len(configs))\n",
    "n_epochs = 25\n",
    "criterion = torch.nn.MSELoss()\n",
    "seeds = list(range(25))\n",
    "batch_size = 128\n",
    "patience = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What impacts overfitting here ?\n",
    "* Optimizer - Adam fits faster and more than SGD\n",
    "* Batch size - Lower batch size seems to lead to more overfitting. Larger ones seem to average out extremes in the input dataset during backprop\n",
    "* Net width - Obviously\n",
    "* Hidden layers - Obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for config in tqdm(configs):\n",
    "        n_layers = config[\"hidden\"]\n",
    "        width = config[\"width\"]\n",
    "        n_obs = config[\"n_obs\"]\n",
    "        decay = config[\"decay\"]\n",
    "        dataset = config[\"dataset\"]\n",
    "        lr = config[\"lr\"]\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        exp_dir = f\"{dataset}/{width=}_{n_layers=}_{n_obs=}_{decay=}_{lr=}\"\n",
    "        print(f\"doing {config}\")\n",
    "        # Train for 25 seeds\n",
    "        for i, seed in enumerate(seeds):\n",
    "            logdir = f\"runs/{exp_dir}/{seed=}\"\n",
    "            writer = SummaryWriter(log_dir=logdir)\n",
    "            min_val_loss = float(\"inf\")\n",
    "\n",
    "            seed_train_losses = [np.nan] * n_epochs\n",
    "            seed_val_losses = [np.nan] * n_epochs\n",
    "            early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "            make_deterministic(seed=seed)\n",
    "\n",
    "            trainloader, X_train, y_train, X_val, y_val, n_dim = setup_data(\n",
    "                dataset, batch_size, n_obs\n",
    "            )\n",
    "\n",
    "            net = Network(n_dim, n_layers, width).to(device)\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "            ### RECORD MODEL ###\n",
    "            writer.add_graph(net, X_train[0])\n",
    "\n",
    "            for e in range(n_epochs):\n",
    "                ### TRAIN ###\n",
    "                for X, y in trainloader:\n",
    "                    optim.zero_grad()\n",
    "                    train_activ = net(X)\n",
    "                    train_loss = criterion(train_activ, y)\n",
    "                    train_loss.backward()\n",
    "                    optim.step()\n",
    "\n",
    "                ### RECORD WEIGHTS FOR VIZ ###\n",
    "                for name, weight in net.named_parameters():\n",
    "                    writer.add_histogram(name, weight, e)\n",
    "                    writer.add_histogram(f\"{name}.grad\", weight.grad, e)\n",
    "\n",
    "                ### EVAL ###\n",
    "                with torch.no_grad():\n",
    "                    val_activ = net(X_val)\n",
    "                    val_loss = criterion(val_activ, y_val).item()\n",
    "                    seed_val_losses[e] = val_loss\n",
    "\n",
    "                    train_activ = net(X_train)\n",
    "                    train_loss = criterion(train_activ, y_train).item()\n",
    "                    seed_train_losses[e] = train_loss\n",
    "\n",
    "                    writer.add_scalar(\"Loss/train\", train_loss, e)\n",
    "                    writer.add_scalar(\"Loss/val\", val_loss, e)\n",
    "                    if val_loss < min_val_loss:\n",
    "                        val_activ_min_loss = val_activ.detach().clone().cpu().numpy()\n",
    "                        train_activ_min_loss = train_activ.detach().clone().cpu().numpy()\n",
    "\n",
    "                ### VERIFY EARLY STOP ###\n",
    "                # Is weird rn but basically we just want to record the first early stop activations, but since we also want the lowest validation error's activation we can't break out yet\n",
    "                if not early_stopping.early_stop:\n",
    "                    early_stopping(val_loss, train_activ, val_activ)\n",
    "                    if early_stopping.early_stop:\n",
    "                        ### PLOT EARLY STOP REPRESENTATION ###\n",
    "                        train_activ_graph_early_stop = (\n",
    "                            early_stopping.train_activ.cpu().numpy()\n",
    "                        )\n",
    "                        val_activ_graph_early_stop = early_stopping.val_activ.cpu().numpy()\n",
    "                        fig_pgt_train = plot_pred_vs_gt(\n",
    "                            y_train.cpu().numpy(),\n",
    "                            train_activ_graph_early_stop,\n",
    "                            title=\"Prédiction par rapport à la vérité (es) (entrainement)\",\n",
    "                        )\n",
    "\n",
    "                        fig_pgt_val = plot_pred_vs_gt(\n",
    "                            y_val.cpu().numpy(),\n",
    "                            val_activ_graph_early_stop,\n",
    "                            title=\"Prédiction par rapport à la vérité (es) (validation)\",\n",
    "                        )\n",
    "                        writer.add_figure(\"train_pred_vs_gt_es\", fig_pgt_train)\n",
    "                        writer.add_figure(\"val_pred_vs_gt_es\", fig_pgt_val)\n",
    "\n",
    "            ### PLOT PRED VS TRUE FOR THIS SEED  (min loss) ###\n",
    "            fig_pgt_train = plot_pred_vs_gt(\n",
    "                y_train.cpu().numpy(),\n",
    "                train_activ_min_loss,\n",
    "                title=\"Prédiction par rapport à la vérité (min) (entrainement)\",\n",
    "            )\n",
    "\n",
    "            fig_pgt_val = plot_pred_vs_gt(\n",
    "                y_val.cpu().numpy(),\n",
    "                val_activ_min_loss,\n",
    "                title=\"Prédiction par rapport à la vérité (min) (validation)\",\n",
    "            )\n",
    "            writer.add_figure(\"train_pred_vs_gt_min\", fig_pgt_train)\n",
    "            writer.add_figure(\"val_pred_vs_gt_min\", fig_pgt_val)\n",
    "            writer.flush()\n",
    "            writer.close()\n",
    "\n",
    "            train_losses.append(seed_train_losses)\n",
    "            val_losses.append(seed_val_losses)\n",
    "\n",
    "        ### PLOT AGGREGATE DATA FOR ALL SEEDS ###\n",
    "        logdir = f\"runs/{dataset}/{width=}_{n_layers=}_{n_obs=}_{decay=}_{lr=}/aggregate\"\n",
    "        writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "        ### PLOTS ###\n",
    "        train_losses = np.array(train_losses)\n",
    "        val_losses = np.array(val_losses)\n",
    "        fig_loss = plot_losses(n_epochs, train_losses, val_losses)\n",
    "        writer.add_figure(\"losses\", fig_loss)\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "        save_metrics(train_losses, f\"metrics/{exp_dir}/train_losses\")\n",
    "        save_metrics(val_losses, f\"metrics/{exp_dir}/val_losses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_74290/2109861781.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 100, 'dataset': '50_rx_100000_combis_4_patterns_3', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:07<19:07, 127.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 100, 'dataset': '100_rx_100000_combis_10_patterns_35', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:23<17:38, 132.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 100, 'dataset': '1000_rx_100000_combis_10_patterns_25', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [11:26<30:56, 265.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 1000, 'dataset': '50_rx_100000_combis_4_patterns_3', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [13:39<21:17, 212.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 1000, 'dataset': '100_rx_100000_combis_10_patterns_35', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [16:07<15:47, 189.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 1000, 'dataset': '1000_rx_100000_combis_10_patterns_25', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [22:50<17:29, 262.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 10000, 'dataset': '50_rx_100000_combis_4_patterns_3', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [27:18<13:11, 263.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 10000, 'dataset': '100_rx_100000_combis_10_patterns_35', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [32:00<08:59, 269.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 1, 'n_obs': 10000, 'dataset': '1000_rx_100000_combis_10_patterns_25', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [41:46<06:08, 368.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing {'width': 32, 'hidden': 2, 'n_obs': 100, 'dataset': '50_rx_100000_combis_4_patterns_3', 'decay': 0, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [44:00<00:00, 264.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Profile printout saved to text file mprof0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
