{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "# from utils import *\n",
    "from fit_sup_utils import *\n",
    "from networks import Network, VariableNet\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # default = (6.4, 4.8)\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 140  # default = 100\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "title_font_size = \"10\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 1.1\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What affects overfitting here ?\n",
    "* Optimizer - Adam fits faster and more than SGD\n",
    "* Learning rate - Lower (0.001) seems better than default (1e-2)\n",
    "* Batch size - Lower batch size seems to lead to more overfitting. Larger ones seem to average out extremes in the input dataset during backprop\n",
    "* Net width - Obviously\n",
    "* Hidden layers - Obviously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What helps fitting \"outliers\" ?\n",
    "Fitting is meant in a broad sense here and just means \"Not predicting the mean\"\n",
    "\n",
    "* Lower LR (?) 0.001 is better than 0.01\n",
    "* Label dist. smoothing, but seems to affect validation perf for the \"common\" cluster (not outlier)\n",
    "* Quantile loss seems to help over estimating values properly when using a quantile at 0.75 (i.e. low risk cluster is overestimated to a lesser extent than high risk cluster, which could be good for the bandit algorithm), HOWEVER, fitting a quantile that isn't 50 feels like it could mess up with NeuralTS (since we're not predicting a mean anymore, we're predicting a quantile, and the output of the NN usually goes into a Normal distribution as the mean param)\n",
    "* Label dist smoothing with a bigger batch size seems to help \n",
    "* More data. You need to have at least a couple of high risk observations in order to get a good idea of what makes a high risk combination\n",
    "* for 1000 rx, small batches, sqrt_inv LDS, smaller models seems to have less bias in validation\n",
    "* Low dim, MSE is sufficient, high dim, quantile seems better in early \n",
    "* 3 layers of 128 width seem to be sufficient to overfit in training\n",
    "* lr of 0.001 seem to help that overfitting, nice, lr of 0.01 seems to fail overfitting in some cases (dataset 100 and 50)\n",
    "* mse is better than rmse\n",
    "* If doing LDS, sqrt_inv is better than just True\n",
    "* For low dim data: MSE seems equal or better than quantile in more situations (learning rate for example)\n",
    "* Avoir un validation set pondere de maniere plus egale (ex. bins et extrema)\n",
    "\n",
    "* **LDS WORKS, IMPORTANT, USE IT**\n",
    "* **EXTREMA VALIDATION IS BETTER THAN BINS**\n",
    "* **WITH VAL EXTREMA, MSE PERFORMS SIMILARLY TO QUANTILE, BUT IS SIMPLER**\n",
    "* **IF WE USE WEIGHT DECAY THAT DECREASES WHEN EPOCHS GO UP, FOCUS ON TRAINING LOSS, WE CAN GET VERY GOOD RESULTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does not work\n",
    "\n",
    "* l1 loss\n",
    "* High LR (0.01) seems to lead to high bias in validation\n",
    "* Low observation counts\n",
    "* Embedding combinations with an AE\n",
    "* Adaptive LR with high start LR (the LR goes down too fast too soon and underfits completely)\n",
    "* High batch size tends to smoothen out the less populated clusters (high risk in this case, which contains important information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General observations\n",
    "* Quantile loss seems to work better with LDS\n",
    "* More data always helps. Also, the amount of data isn't a problem in our setting, the computation of the label is.\n",
    "* The more dimensions we have, the less effective having a validation set seems to be. Possible explanation: having a training set which is representative of the validation set in some way gets harder the more dimensions we have, therefore, validation loss can hardly go down, so training stops early and cannot even learn a good representation.\n",
    "* For 50 dims, 10000 obs is enough\n",
    "* Extrema seems better in general than bins for validation set since bins tends to get a representation which diminishes the loss at the mean of the dataset, which is easy, but not helpful\n",
    "* Decrease decay as epochs go on seems to work REALLY well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipes\n",
    "* 1\n",
    "    * decay based on epoch + batch_norm + lr 0.001 + loss = mse + you can play around with batch_size, bigger works well + label dist smoothing True\n",
    "    * width=128/hidden=3/n_obs=10000/decay=epoch/lr=0.01/custom_layers=None/reweight=True/batch_size=1024/dropout_rate=None/loss=\\['mse']/classif_thresh=None/batch_norm=True/patience=25/validation=extrema/seed=0\n",
    "    * dataset=100_rx_100000_combis_10_patterns_35/width=128/hidden=3/n_obs=20000/decay=epoch/lr=plateau/custom_layers=None/reweight=True/batch_size=1024/dropout_rate=None/loss=['mse']/classif_thresh=None/batch_norm=True/patience=15/validation=extrema/modif=4096/seed=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To try\n",
    "* ~~Very simple network with LDS (like one that can hardly overfit)~~ That alone doesnt work that well\n",
    "* Transform regression into a classification with the new knowledge (how to interpret CIs then ?)\n",
    "* ~~High batch size with sqrt_inv LDS and lower LR~~ Seems to work alright, but tends to smoothen out outliers because of big batches\n",
    "* ~~Adaptive LR (Plateau)~~ Does not always work well, but seems sound to try in general\n",
    "* ~~Embed combination vectors. Each RX is a word, each combination of Rx is like a sentence. Embedding should pick up a relationship between the Rxs~~ Does not work well\n",
    "* ~~50 rx and 100 rx with less than 10000 observations for a good architecture~~ Does not work well\n",
    "* ~~Extrema with 0.001 decay~~\n",
    "* ~~Early stop on r2 score instead of val loss~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Generalize in validation\n",
    "* If unable to generalize in the conventional sense, at least make sure the \"high risk\" cluster is estimated over the \"low risk\" cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good models\n",
    "*  dataset=1000_rx_100000_combis_10_patterns_25/width=128/hidden=4/n_obs=40000/decay=0/lr=0.001/custom_layers=None/reweight=sqrt_inv/batch_size=32/dropout_rate=None/loss=['quantile', [0.3, 0.5, 0.7]]/classif_thresh=None/batch_norm=True/patience=25/validation=bins/seed=0 \n",
    "* dataset=100_rx_100000_combis_10_patterns_35/width=128/hidden=4/n_obs=30000/decay=0/lr=0.001/custom_layers=None/reweight=None/batch_size=32/dropout_rate=None/loss=\\['mse'\\]/classif_thresh=None/batch_norm=True/patience=10/validation=bins/seed=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "# seeds = list(range(25))\n",
    "seeds = [0]\n",
    "param_values = {\n",
    "    \"dataset\": [\n",
    "        \"50_rx_100000_combis_4_patterns_3\",\n",
    "        \"100_rx_100000_combis_10_patterns_35\",\n",
    "        # \"1000_rx_100000_combis_10_patterns_25\",\n",
    "    ],\n",
    "    \"width\": [128],\n",
    "    \"hidden\": [3],\n",
    "    \"n_obs\": [20000],\n",
    "    \"decay\": [\"epoch\"],\n",
    "    \"lr\": [\"plateau\"],\n",
    "    \"custom_layers\": [None],\n",
    "    \"reweight\": [\"sqrt_inv\"],\n",
    "    \"batch_size\": [1024],\n",
    "    \"dropout_rate\": [None],\n",
    "    \"loss\": [[\"mse\"]],\n",
    "    \"classif_thresh\": [None],\n",
    "    \"batch_norm\": [True],\n",
    "    \"patience\": [25],\n",
    "    \"validation\": [\"extrema\"],\n",
    "    \"modif\":[\"4096\"]\n",
    "}\n",
    "configs = [dict(zip(param_values, v)) for v in product(*param_values.values())]\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(config, exp_dir=\"test_epochdecay_n_samples\"):\n",
    "    n_layers = config[\"hidden\"]\n",
    "    width = config[\"width\"]\n",
    "    n_obs = config[\"n_obs\"]\n",
    "    decay = config[\"decay\"]\n",
    "    dataset = config[\"dataset\"]\n",
    "    lr = config[\"lr\"]\n",
    "    custom_layers = config[\"custom_layers\"]\n",
    "    reweight = config[\"reweight\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    loss_info = config[\"loss\"]\n",
    "    classif_thresh = config[\"classif_thresh\"]\n",
    "    batch_norm = config[\"batch_norm\"]\n",
    "    patience = config[\"patience\"]\n",
    "    validation = config[\"validation\"]\n",
    "\n",
    "    n_outputs = 1\n",
    "    pred_idx = 0\n",
    "\n",
    "    criterion = get_loss(*loss_info)\n",
    "    if loss_info[0] == \"quantile\":\n",
    "        quantiles = np.array(loss_info[1])\n",
    "        assert 0.5 in quantiles\n",
    "        pred_idx = np.where(quantiles == 0.5)[0][0]\n",
    "        n_outputs = len(quantiles)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_losses = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    test_r2s = []\n",
    "    if exp_dir == None:\n",
    "        exp_dir == \"\"\n",
    "    l = []\n",
    "    for k, v in config.items():\n",
    "        l += [f\"{k}={v}\"]\n",
    "\n",
    "    exp_dir += \"/\" + \"/\".join(l)\n",
    "\n",
    "    # Train for 25 seeds\n",
    "    for i, seed in enumerate(seeds):\n",
    "        logdir = f\"runs/{exp_dir}/{seed=}\"\n",
    "        writer = SummaryWriter(log_dir=logdir)\n",
    "        min_val_loss = float(\"inf\")\n",
    "        min_train_loss = float(\"inf\")\n",
    "        val_activ_min_loss = None\n",
    "        train_activ_min_loss = None\n",
    "        test_activ_min_loss = None\n",
    "        val_activ_mintrain_loss = None\n",
    "        train_activ_mintrain_loss = None\n",
    "        test_activ_mintrain_loss = None\n",
    "        mintrain_epoch = 0\n",
    "        minval_epoch = 0\n",
    "\n",
    "        seed_train_losses = [np.nan] * n_epochs\n",
    "        seed_val_losses = [np.nan] * n_epochs\n",
    "        seed_test_losses = [np.nan] * n_epochs\n",
    "        seed_train_r2s = [np.nan] * n_epochs\n",
    "        seed_val_r2s = [np.nan] * n_epochs\n",
    "        seed_test_r2s = [np.nan] * n_epochs\n",
    "        early_stopping = EarlyStoppingActiv(patience=patience)\n",
    "\n",
    "        make_deterministic(seed=seed)\n",
    "\n",
    "        trainloader, training_data, X_val, y_val, n_dim, X_test, y_test = setup_data(\n",
    "            dataset, batch_size, n_obs, reweight, classif_thresh, validation\n",
    "        )\n",
    "\n",
    "        X_train, y_train = training_data.combis, training_data.labels\n",
    "        if custom_layers is not None:\n",
    "            net = VariableNet(n_dim, custom_layers)\n",
    "        else:\n",
    "            net = Network(\n",
    "                n_dim, n_layers, n_outputs, width, dropout_rate, batch_norm\n",
    "            ).to(device)\n",
    "\n",
    "\n",
    "        if decay == \"epoch\":\n",
    "            decay_val = 1\n",
    "        else:\n",
    "            decay_val = decay\n",
    "\n",
    "        if lr == \"plateau\":\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=decay_val)\n",
    "            sched = ReduceLROnPlateau(optim, \"min\", patience=patience - 2)\n",
    "        else:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=decay_val)\n",
    "\n",
    "\n",
    "        ### RECORD MODEL ###\n",
    "        writer.add_graph(net, X_train)\n",
    "\n",
    "        for e in range(n_epochs):\n",
    "            if decay == \"epoch\":\n",
    "                optim.param_groups[0][\"weight_decay\"] = 1 / (e + 1)\n",
    "                \n",
    "            ### TRAIN ###\n",
    "            for X, y in trainloader:\n",
    "                optim.zero_grad()\n",
    "                train_activ = net(X)\n",
    "                train_loss = criterion(train_activ, y)\n",
    "                if loss_info[0] == \"rmse\":\n",
    "                    train_loss = torch.sqrt(train_loss)\n",
    "                train_loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            ### EVAL ###\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                # Compute losses\n",
    "                (\n",
    "                    train_activ,\n",
    "                    train_loss,\n",
    "                    val_activ,\n",
    "                    val_loss,\n",
    "                    test_activ,\n",
    "                    test_loss,\n",
    "                ) = get_losses_and_activ(\n",
    "                    net,\n",
    "                    criterion,\n",
    "                    loss_info,\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_val,\n",
    "                    y_val,\n",
    "                    X_test,\n",
    "                    y_test,\n",
    "                )\n",
    "                net.train()\n",
    "\n",
    "                # Get R2 metric\n",
    "                train_r2 = r2_score(\n",
    "                    y_train.cpu().numpy(), train_activ.cpu().numpy()[:, pred_idx]\n",
    "                )\n",
    "                val_r2 = r2_score(\n",
    "                    y_val.cpu().numpy(), val_activ.cpu().numpy()[:, pred_idx]\n",
    "                )\n",
    "\n",
    "                # Save\n",
    "                seed_train_losses[e] = train_loss\n",
    "                seed_val_losses[e] = val_loss\n",
    "                seed_train_r2s[e] = train_r2\n",
    "                seed_val_r2s[e] = val_r2\n",
    "\n",
    "                writer.add_scalar(\"Loss/train\", train_loss, e)\n",
    "                writer.add_scalar(\"Loss/val\", val_loss, e)\n",
    "                writer.add_scalar(\"R2/train\", train_r2, e)\n",
    "                writer.add_scalar(\"R2/val\", val_r2, e)\n",
    "\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    test_r2 = r2_score(\n",
    "                        y_test.cpu().numpy(), test_activ.cpu().numpy()[:, pred_idx]\n",
    "                    )\n",
    "                    seed_test_losses[e] = test_loss\n",
    "                    seed_test_r2s[e] = test_r2\n",
    "                    writer.add_scalar(\"Loss/test\", test_loss, e)\n",
    "                    writer.add_scalar(\"R2/test\", test_r2, e)\n",
    "\n",
    "                # Update LR scheduler\n",
    "                if type(lr) == str:\n",
    "                    sched.step(val_loss)\n",
    "\n",
    "                (\n",
    "                    train_activ_min_loss,\n",
    "                    val_activ_min_loss,\n",
    "                    test_activ_min_loss,\n",
    "                    min_val_loss,\n",
    "                    train_activ_mintrain_loss,\n",
    "                    val_activ_mintrain_loss,\n",
    "                    test_activ_mintrain_loss,\n",
    "                    min_train_loss,\n",
    "                    mintrain_epoch,\n",
    "                    minval_epoch,\n",
    "                ) = update_minimums(\n",
    "                    train_loss,\n",
    "                    min_train_loss,\n",
    "                    train_activ,\n",
    "                    val_loss,\n",
    "                    min_val_loss,\n",
    "                    val_activ,\n",
    "                    test_loss,\n",
    "                    test_activ,\n",
    "                    pred_idx,\n",
    "                    val_activ_min_loss,\n",
    "                    train_activ_min_loss,\n",
    "                    test_activ_min_loss,\n",
    "                    val_activ_mintrain_loss,\n",
    "                    train_activ_mintrain_loss,\n",
    "                    test_activ_mintrain_loss,\n",
    "                    e,\n",
    "                    mintrain_epoch,\n",
    "                    minval_epoch\n",
    "                )\n",
    "            ### VERIFY EARLY STOP ###\n",
    "            # Is weird rn but basically we just want to record the first early stop activations, but since we also want the lowest validation error's activation we can't break out yet\n",
    "            if not early_stopping.early_stop:\n",
    "                early_stopping(val_loss, train_activ, val_activ, test_activ)\n",
    "                if early_stopping.early_stop:\n",
    "                    ### PLOT EARLY STOP REPRESENTATION ###\n",
    "                    fig_pgt_es = plot_pred_vs_gt(\n",
    "                        y_train,\n",
    "                        early_stopping.train_activ,\n",
    "                        y_val,\n",
    "                        early_stopping.val_activ,\n",
    "                        y_test, \n",
    "                        early_stopping.test_activ,\n",
    "                        title=f\"Prédiction par rapport à la vérité (epoch {e})(Early Stop)\",\n",
    "                        pred_idx=pred_idx\n",
    "                    )\n",
    "                    writer.add_figure(\"pred_vs_gt_early_stop\", fig_pgt_es)\n",
    "\n",
    "                    fig_pgt_es = plot_pred_vs_gt(\n",
    "                        y_train,\n",
    "                        early_stopping.train_activ,\n",
    "                        y_val,\n",
    "                        early_stopping.val_activ,\n",
    "                        y_test, \n",
    "                        early_stopping.test_activ,\n",
    "                        title=f\"Prédiction par rapport à la vérité (epoch {e})(Early Stop)\",\n",
    "                        pred_idx=pred_idx,\n",
    "                        invert=True\n",
    "                    )\n",
    "                    writer.add_figure(\"zinvert_pred_vs_gt_early_stop\", fig_pgt_es)\n",
    "                    writer.flush()\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min val loss) ###\n",
    "        fig_pgt_minval = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_min_loss,\n",
    "            y_val,\n",
    "            val_activ_min_loss,\n",
    "            y_test,\n",
    "            test_activ_min_loss,\n",
    "            title=f\"Prédiction par rapport à la vérité (epoch {minval_epoch})(min val)\",\n",
    "            pred_idx=pred_idx\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"pred_vs_gt_minval\", fig_pgt_minval)\n",
    "\n",
    "        fig_pgt_minval = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_min_loss,\n",
    "            y_val,\n",
    "            val_activ_min_loss,\n",
    "            y_test,\n",
    "            test_activ_min_loss,\n",
    "            title=f\"Prédiction par rapport à la vérité (epoch {minval_epoch})(min val)\",\n",
    "            pred_idx=pred_idx,\n",
    "            invert=True\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"zinvert_pred_vs_gt_minval\", fig_pgt_minval)\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min train loss) ###\n",
    "        fig_pgt_mintrain = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_mintrain_loss,\n",
    "            y_val,\n",
    "            val_activ_mintrain_loss,\n",
    "            y_test,\n",
    "            test_activ_mintrain_loss,\n",
    "            title=f\"Prédiction par rapport à la vérité (epoch {mintrain_epoch})(min train)\",\n",
    "            pred_idx=pred_idx\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"pred_vs_gt_mintrain\", fig_pgt_mintrain)\n",
    "\n",
    "        fig_pgt_mintrain = plot_pred_vs_gt(\n",
    "            y_train,\n",
    "            train_activ_mintrain_loss,\n",
    "            y_val,\n",
    "            val_activ_mintrain_loss,\n",
    "            y_test,\n",
    "            test_activ_mintrain_loss,\n",
    "            title=f\"Prédiction par rapport à la vérité (epoch {mintrain_epoch})(min train)\",\n",
    "            pred_idx=pred_idx,\n",
    "            invert=True\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"zinvert_pred_vs_gt_mintrain\", fig_pgt_mintrain)\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        train_losses.append(seed_train_losses)\n",
    "        val_losses.append(seed_val_losses)\n",
    "        test_losses.append(seed_test_losses)\n",
    "        train_r2s.append(seed_train_r2s)\n",
    "        val_r2s.append(seed_val_r2s)\n",
    "        test_r2s.append(seed_test_r2s)\n",
    "    ### PLOT AGGREGATE DATA FOR ALL SEEDS ###\n",
    "    # logdir = f\"runs/{exp_dir}/aggregate\"\n",
    "    # writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    # ### PLOTS ###\n",
    "    # train_losses = np.array(train_losses)\n",
    "    # val_losses = np.array(val_losses)\n",
    "    # test_losses = np.array(test_losses)\n",
    "    # fig_loss =  plot_metric(n_epochs, train_losses, val_losses, test_losses)\n",
    "\n",
    "    # train_r2s = np.array(train_r2s)\n",
    "    # val_r2s = np.array(val_r2s)\n",
    "    # fig_r2 = plot_metric(n_epochs, train_r2s, val_r2s, test_r2s)\n",
    "\n",
    "    # writer.add_figure(\"losses\", fig_loss)\n",
    "    # writer.add_figure(\"r2s\", fig_r2)\n",
    "\n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    save_metrics(train_losses, f\"metrics/{exp_dir}/train_losses\")\n",
    "    save_metrics(val_losses, f\"metrics/{exp_dir}/val_losses\")\n",
    "    save_metrics(test_losses, f\"metrics/{exp_dir}/test_losses\")\n",
    "    save_metrics(train_r2s, f\"metrics/{exp_dir}/train_r2s\")\n",
    "    save_metrics(val_r2s, f\"metrics/{exp_dir}/val_r2s\")\n",
    "    save_metrics(test_r2s, f\"metrics/{exp_dir}/test_r2s\")\n",
    "\n",
    "    # print(f\"saved at runs/{exp_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    run_config(config)\n",
    "\n",
    "# with Pool(4) as p:\n",
    "#     p.map(run_config, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
