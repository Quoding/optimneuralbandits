{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from fit_sup_utils import *\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # default = (6.4, 4.8)\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 140  # default = 100\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "title_font_size = \"10\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 1.1\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "param_values = {\n",
    "    \"dataset\": [\n",
    "        \"50_rx_100000_combis_4_patterns_3\",\n",
    "        \"100_rx_100000_combis_10_patterns_35\",\n",
    "        \"1000_rx_100000_combis_10_patterns_25\",\n",
    "    ],\n",
    "    \"width\": [64, 128, 256],\n",
    "    \"hidden\": [3, 4, 5],\n",
    "    \"n_obs\": [20000],\n",
    "    \"decay\": [0],\n",
    "    \"lr\": [1e-2],\n",
    "    \"custom_layers\": [None],\n",
    "    \"reweight\": [None, \"sqrt_inv\"],\n",
    "    \"batch_size\": [32],\n",
    "    \"dropout_rate\" : [None],\n",
    "    \"loss\": [\"rmse\", \"mse\"]\n",
    "}\n",
    "\n",
    "configs = [dict(zip(param_values, v)) for v in product(*param_values.values())]\n",
    "\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What affects overfitting here ?\n",
    "* Optimizer - Adam fits faster and more than SGD\n",
    "* Batch size - Lower batch size seems to lead to more overfitting. Larger ones seem to average out extremes in the input dataset during backprop\n",
    "* Net width - Obviously\n",
    "* Hidden layers - Obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "# seeds = list(range(25))\n",
    "seeds = [0]\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(config, exp_dir=\"test_r2\", modifier=\"\"):\n",
    "    n_layers = config[\"hidden\"]\n",
    "    width = config[\"width\"]\n",
    "    n_obs = config[\"n_obs\"]\n",
    "    decay = config[\"decay\"]\n",
    "    dataset = config[\"dataset\"]\n",
    "    lr = config[\"lr\"]\n",
    "    custom_layers = config[\"custom_layers\"]\n",
    "    reweight = config[\"reweight\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    loss_name = config[\"loss\"]\n",
    "\n",
    "    criterion = get_loss(loss_name)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    if exp_dir == None:\n",
    "        exp_dir == \"\"    \n",
    "    l = []\n",
    "    for k, v in config.items():\n",
    "        l += [f\"{k}={v}\"]\n",
    "\n",
    "    exp_dir += \"/\" + \"-\".join(l) + f\"-{modifier=}\"\n",
    "\n",
    "    # Train for 25 seeds\n",
    "    for i, seed in enumerate(seeds):\n",
    "        logdir = f\"runs/{exp_dir}/{seed=}\"\n",
    "        writer = SummaryWriter(log_dir=logdir)\n",
    "        min_val_loss = float(\"inf\")\n",
    "        min_train_loss = float(\"inf\")\n",
    "\n",
    "        seed_train_losses = [np.nan] * n_epochs\n",
    "        seed_val_losses = [np.nan] * n_epochs\n",
    "        seed_train_r2s = [np.nan] * n_epochs\n",
    "        seed_val_r2s = [np.nan] * n_epochs\n",
    "        early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "        make_deterministic(seed=seed)\n",
    "\n",
    "        trainloader, X_train, y_train, X_val, y_val, n_dim = setup_data(\n",
    "            dataset, batch_size, n_obs, reweight\n",
    "        )\n",
    "        if custom_layers is not None:\n",
    "            net = VariableNet(n_dim, custom_layers)\n",
    "        else:\n",
    "            net = Network(n_dim, n_layers, width, dropout_rate).to(device)\n",
    "        optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "\n",
    "        ### RECORD MODEL ###\n",
    "        writer.add_graph(net, X_train[0])\n",
    "\n",
    "        for e in range(n_epochs):\n",
    "            ### TRAIN ###\n",
    "            for X, y in trainloader:\n",
    "                optim.zero_grad()\n",
    "                train_activ = net(X)\n",
    "                train_loss = criterion(train_activ, y)\n",
    "                if loss_name == \"rmse\":\n",
    "                    train_loss = torch.sqrt(train_loss)\n",
    "                train_loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            ### EVAL ###\n",
    "            with torch.no_grad():\n",
    "                # Compute losses\n",
    "                val_activ = net(X_val)\n",
    "                val_loss = criterion(val_activ, y_val)\n",
    "\n",
    "                if loss_name == \"rmse\":\n",
    "                    val_loss = torch.sqrt(val_loss)\n",
    "\n",
    "                train_activ = net(X_train)\n",
    "                train_loss = criterion(train_activ, y_train)\n",
    "\n",
    "                if loss_name == \"rmse\":\n",
    "                    train_loss = torch.sqrt(train_loss)\n",
    "\n",
    "                val_loss = val_loss.item()\n",
    "                train_loss = train_loss.item()\n",
    "                \n",
    "                # Get R2 metric\n",
    "                train_r2 = r2_score(y_train.cpu().numpy(), train_activ.cpu().numpy())\n",
    "                val_r2 = r2_score(y_val.cpu().numpy(), val_activ.cpu().numpy())\n",
    "\n",
    "                # Save\n",
    "                seed_train_losses[e] = train_loss\n",
    "                seed_val_losses[e] = val_loss\n",
    "                seed_train_r2s[e] = train_r2\n",
    "                seed_val_r2s[e] = val_r2\n",
    "\n",
    "                writer.add_scalar(\"Loss/train\", train_loss, e)\n",
    "                writer.add_scalar(\"Loss/val\", val_loss, e)\n",
    "                writer.add_scalar(\"R2/train\", train_r2, e)\n",
    "                writer.add_scalar(\"R2/val\", val_r2, e)\n",
    "\n",
    "                # Update minimums\n",
    "                if val_loss < min_val_loss:\n",
    "                    val_activ_min_loss = val_activ.detach().clone().cpu().numpy()\n",
    "                    train_activ_min_loss = train_activ.detach().clone().cpu().numpy()\n",
    "                    min_val_loss = val_loss\n",
    "                if train_loss < min_train_loss:\n",
    "                    val_activ_mintrain_loss = val_activ.detach().clone().cpu().numpy()\n",
    "                    train_activ_mintrain_loss = (\n",
    "                        train_activ.detach().clone().cpu().numpy()\n",
    "                    )\n",
    "                    min_train_loss = train_loss\n",
    "\n",
    "            ### VERIFY EARLY STOP ###\n",
    "            # Is weird rn but basically we just want to record the first early stop activations, but since we also want the lowest validation error's activation we can't break out yet\n",
    "            if not early_stopping.early_stop:\n",
    "                early_stopping(val_loss, train_activ, val_activ)\n",
    "                if early_stopping.early_stop:\n",
    "                    ### PLOT EARLY STOP REPRESENTATION ###\n",
    "                    train_activ_graph_early_stop = (\n",
    "                        early_stopping.train_activ.cpu().numpy()\n",
    "                    )\n",
    "                    val_activ_graph_early_stop = early_stopping.val_activ.cpu().numpy()\n",
    "                    fig_pgt_train = plot_pred_vs_gt(\n",
    "                        y_train.cpu().numpy(),\n",
    "                        train_activ_graph_early_stop,\n",
    "                        title=\"Prédiction par rapport à la vérité (es) (entrainement)\",\n",
    "                    )\n",
    "\n",
    "                    fig_pgt_val = plot_pred_vs_gt(\n",
    "                        y_val.cpu().numpy(),\n",
    "                        val_activ_graph_early_stop,\n",
    "                        title=\"Prédiction par rapport à la vérité (es) (validation)\",\n",
    "                    )\n",
    "                    writer.add_figure(\"train_pred_vs_gt_es\", fig_pgt_train)\n",
    "                    writer.add_figure(\"val_pred_vs_gt_es\", fig_pgt_val)\n",
    "                    writer.flush()\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min loss) ###\n",
    "        fig_pgt_train = plot_pred_vs_gt(\n",
    "            y_train.cpu().numpy(),\n",
    "            train_activ_min_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (minval) (entrainement)\",\n",
    "        )\n",
    "\n",
    "        fig_pgt_val = plot_pred_vs_gt(\n",
    "            y_val.cpu().numpy(),\n",
    "            val_activ_min_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (minval) (validation)\",\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"train_pred_vs_gt_minval\", fig_pgt_train)\n",
    "        writer.add_figure(\"val_pred_vs_gt_minval\", fig_pgt_val)\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min loss) ###\n",
    "        fig_pgt_train_mintrain = plot_pred_vs_gt(\n",
    "            y_train.cpu().numpy(),\n",
    "            train_activ_mintrain_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (mintrain) (entrainement)\",\n",
    "        )\n",
    "\n",
    "        fig_pgt_val_mintrain = plot_pred_vs_gt(\n",
    "            y_val.cpu().numpy(),\n",
    "            val_activ_mintrain_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (mintrain) (validation)\",\n",
    "        )\n",
    "\n",
    "        print(val_activ_min_loss.min())\n",
    "        print(val_activ_min_loss.max())\n",
    "        writer.add_figure(\"train_pred_vs_gt_mintrain\", fig_pgt_train_mintrain)\n",
    "        writer.add_figure(\"val_pred_vs_gt_mintrain\", fig_pgt_val_mintrain)\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        train_losses.append(seed_train_losses)\n",
    "        val_losses.append(seed_val_losses)\n",
    "        train_r2s.append(seed_train_r2s)\n",
    "        val_r2s.append(seed_val_r2s)\n",
    "    ### PLOT AGGREGATE DATA FOR ALL SEEDS ###\n",
    "    # logdir = f\"runs/{exp_dir}/aggregate\"\n",
    "    # writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    # ### PLOTS ###\n",
    "    # train_losses = np.array(train_losses)\n",
    "    # val_losses = np.array(val_losses)\n",
    "    # fig_loss = plot_losses(n_epochs, train_losses, val_losses)\n",
    "    # train_r2s = np.array(train_r2s)\n",
    "    # val_r2s = np.array(val_r2s)\n",
    "    # fig_r2 = plot_losses(n_epochs, train_r2s, val_r2s)\n",
    "    # writer.add_figure(\"losses\", fig_loss)\n",
    "    # writer.add_figure(\"r2s\", fig_r2)\n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    save_metrics(train_losses, f\"metrics/{exp_dir}/train_losses\")\n",
    "    save_metrics(val_losses, f\"metrics/{exp_dir}/val_losses\")\n",
    "    save_metrics(train_r2s, f\"metrics/{exp_dir}/train_r2s\")\n",
    "    save_metrics(val_r2s, f\"metrics/{exp_dir}/val_r2s\")\n",
    "\n",
    "    print(f\"saved at runs/{exp_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0053544\n",
      "2.2296143\n",
      "saved at runs/test_r2/dataset=50_rx_100000_combis_4_patterns_3-width=64-hidden=3-n_obs=20000-decay=0-lr=0.01-custom_layers=None-reweight=None-batch_size=32-dropout_rate=None-loss=rmse-modifier=''\n",
      "0.97312737\n",
      "1.9419925\n",
      "saved at runs/test_r2/dataset=50_rx_100000_combis_4_patterns_3-width=64-hidden=3-n_obs=20000-decay=0-lr=0.01-custom_layers=None-reweight=None-batch_size=32-dropout_rate=None-loss=mse-modifier=''\n",
      "Using label distribution smoothing\n",
      "0.8219625\n",
      "2.1475658\n",
      "saved at runs/test_r2/dataset=50_rx_100000_combis_4_patterns_3-width=64-hidden=3-n_obs=20000-decay=0-lr=0.01-custom_layers=None-reweight=sqrt_inv-batch_size=32-dropout_rate=None-loss=rmse-modifier=''\n",
      "Using label distribution smoothing\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    run_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
