{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "from torch.nn.functional import conv1d\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "from utils import *\n",
    "from fit_sup_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)  # default = (6.4, 4.8)\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"figure.dpi\"] = 140  # default = 100\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"text.latex.preamble\"] = [r\"\\usepackage{amsmath}\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "title_font_size = \"10\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 1.1\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3508101453.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"embed_dim\": [32, 64]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# param_values = {\n",
    "#     \"dataset\": [\n",
    "#         \"50_rx_100000_combis_4_patterns_3\",\n",
    "#         \"100_rx_100000_combis_10_patterns_35\",\n",
    "#         \"1000_rx_100000_combis_10_patterns_25\",\n",
    "#     ],\n",
    "#     \"width\": [False],\n",
    "#     \"hidden\": [False],\n",
    "#     \"n_obs\": [20000],\n",
    "#     \"decay\": [0],\n",
    "#     \"lr\": [0.001],\n",
    "#     \"custom_layers\": [[512, 256, 128], [128, 64, 32], [64, 32]],\n",
    "#     \"reweight\": [None, True, \"sqrt_inv\"],\n",
    "#     \"batch_size\": [32],\n",
    "#     \"dropout_rate\" : [None],\n",
    "#     \"loss\": [[\"mse\"], [\"rmse\"]]\n",
    "# }\n",
    "\n",
    "param_values = {\n",
    "    \"dataset\": [\n",
    "        # \"50_rx_100000_combis_4_patterns_3\",\n",
    "        \"100_rx_100000_combis_10_patterns_35\",\n",
    "        \"1000_rx_100000_combis_10_patterns_25\",\n",
    "    ],\n",
    "    \"width\": [128, 256],\n",
    "    \"hidden\": [3, 5],\n",
    "    \"n_obs\": [10000, 20000],\n",
    "    \"decay\": [0],\n",
    "    \"lr\": [0.001],\n",
    "    \"custom_layers\": [None],\n",
    "    \"reweight\": [None, \"sqrt_inv\"],\n",
    "    \"batch_size\": [32, 1024],\n",
    "    \"dropout_rate\" : [None],\n",
    "    \"loss\": [[\"mse\"], [\"rmse\"]],\n",
    "    \"classif_thresh\": [None],\n",
    "    \"embed_dim\": [32, 64]\n",
    "}\n",
    "\n",
    "configs = [dict(zip(param_values, v)) for v in product(*param_values.values())]\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What affects overfitting here ?\n",
    "* Optimizer - Adam fits faster and more than SGD\n",
    "* Learning rate - Lower (0.001) seems better than default (1e-2)\n",
    "* Batch size - Lower batch size seems to lead to more overfitting. Larger ones seem to average out extremes in the input dataset during backprop\n",
    "* Net width - Obviously\n",
    "* Hidden layers - Obviously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What helps fitting \"outliers\" ?\n",
    "Fitting is meant in a broad sense here and just means \"Not predicting the mean\"\n",
    "\n",
    "* Lower LR (?) 0.001 is better than 0.01\n",
    "* Label dist. smoothing, but seems to affect validation perf for the \"common\" cluster (not outlier)\n",
    "* Quantile loss seems to help over estimating values properly when using a quantile at 0.75 (i.e. low risk cluster is overestimated to a lesser extent than high risk cluster, which could be good for the bandit algorithm), HOWEVER, fitting a quantile that isn't 50 feels like it could mess up with NeuralTS (since we're not predicting a mean anymore, we're predicting a quantile, and the output of the NN usually goes into a Normal distribution as the mean param)\n",
    "* Label dist smoothing with a bigger batch size seems to help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does not work\n",
    "\n",
    "* l1 loss\n",
    "* High LR (0.01) seems to lead to high bias in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does work\n",
    "* 3 layers of 128 width seem to be sufficient to overfit in training\n",
    "* lr of 0.001 seem to help that overfitting, nice, lr of 0.01 seems to fail overfitting in some cases (dataset 100 and 50)\n",
    "* mse is better than rmse\n",
    "* If doing LDS, sqrt_inv is better than just True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To try\n",
    "* ~~Very simple network with LDS (like one that can hardly overfit)~~\n",
    "* Transform regression into a classification with the new knowledge\n",
    "* High batch size with sqrt_inv LDS and lower LR\n",
    "* Adaptive LR (Plateau)\n",
    "* Embed combination vectors. Each RX is a word, each combination of Rx is like a sentence. Embedding should pick up a relationship between the Rxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "* Generalize in validation\n",
    "* If unable to generalize in the conventional sense, at least make sure the \"high risk\" cluster is estimated over the \"low risk\" cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "# seeds = list(range(25))\n",
    "seeds = [0]\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(config, exp_dir=\"test_ae\", modifier=\"\"):\n",
    "    n_layers = config[\"hidden\"]\n",
    "    width = config[\"width\"]\n",
    "    n_obs = config[\"n_obs\"]\n",
    "    decay = config[\"decay\"]\n",
    "    dataset = config[\"dataset\"]\n",
    "    lr = config[\"lr\"]\n",
    "    custom_layers = config[\"custom_layers\"]\n",
    "    reweight = config[\"reweight\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    dropout_rate = config[\"dropout_rate\"]\n",
    "    loss_name = config[\"loss\"]\n",
    "    classif_thresh = config[\"classif_thresh\"]\n",
    "    embed_dim = config[\"embed_dim\"]\n",
    "\n",
    "    criterion = get_loss(*loss_name)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    \n",
    "    l = []\n",
    "    for k, v in config.items():\n",
    "        l += [f\"{k}={v}\"]\n",
    "\n",
    "    exp_dir += \"/\" + \"-\".join(l) + f\"-{modifier=}\"\n",
    "\n",
    "    # Train for every seed\n",
    "    for i, seed in enumerate(seeds):\n",
    "        logdir = f\"runs/{exp_dir}/{seed=}\"\n",
    "        writer = SummaryWriter(log_dir=logdir)\n",
    "        min_val_loss = float(\"inf\")\n",
    "        min_train_loss = float(\"inf\")\n",
    "\n",
    "        seed_train_losses = [np.nan] * n_epochs\n",
    "        seed_val_losses = [np.nan] * n_epochs\n",
    "        seed_train_r2s = [np.nan] * n_epochs\n",
    "        seed_val_r2s = [np.nan] * n_epochs\n",
    "        early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "        make_deterministic(seed=seed)\n",
    "\n",
    "        trainloader, training_data, X_val, y_val, n_dim = setup_data(\n",
    "            dataset, batch_size, n_obs, reweight\n",
    "        )\n",
    "        X_train, y_train = training_data.combis, training_data.labels\n",
    "        if custom_layers is not None:\n",
    "            net = VariableNet(embed_dim, custom_layers)\n",
    "        else:\n",
    "            net = Network(embed_dim, n_layers, width, dropout_rate).to(device)\n",
    "\n",
    "        if lr == \"plateau\":\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=decay)\n",
    "            sched = ReduceLROnPlateau(optim, 'min')\n",
    "        else:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=decay)\n",
    "        ### RECORD MODEL ###\n",
    "        # writer.add_graph(net, X_train[0])\n",
    "\n",
    "        # AE with 64 dim. embeddings\n",
    "        print(\"training AE...\")\n",
    "        ae = AE(n_dim, embed_dim, get_layers(n_dim, embed_dim))\n",
    "        print(ae)\n",
    "        ae.fit(n_epochs, training_data, X_val, writer, lr=lr, batch_size=batch_size, patience=patience)\n",
    "        for e in range(n_epochs):\n",
    "            ### TRAIN ###\n",
    "            for X, y in trainloader:\n",
    "                optim.zero_grad()\n",
    "                embed = ae.encoder(X)\n",
    "                train_activ = net(embed)\n",
    "                train_loss = criterion(train_activ, y)\n",
    "                if loss_name[0] == \"rmse\":\n",
    "                    train_loss = torch.sqrt(train_loss)\n",
    "                train_loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "            ### EVAL ###\n",
    "            with torch.no_grad():\n",
    "                # Compute losses\n",
    "                embed = ae.encoder(X_val)\n",
    "                val_activ = net(embed)\n",
    "                val_loss = criterion(val_activ, y_val)\n",
    "\n",
    "                embed = ae.encoder(X_train)\n",
    "                train_activ = net(embed)\n",
    "                train_loss = criterion(train_activ, y_train)\n",
    "\n",
    "                if loss_name[0] == \"rmse\":\n",
    "                    train_loss = torch.sqrt(train_loss)\n",
    "                    val_loss = torch.sqrt(val_loss)\n",
    "\n",
    "                val_loss = val_loss.item()\n",
    "                train_loss = train_loss.item()\n",
    "                \n",
    "                # Get R2 metric\n",
    "                train_r2 = r2_score(y_train.cpu().numpy(), train_activ.cpu().numpy())\n",
    "                val_r2 = r2_score(y_val.cpu().numpy(), val_activ.cpu().numpy())\n",
    "\n",
    "                # Save\n",
    "                seed_train_losses[e] = train_loss\n",
    "                seed_val_losses[e] = val_loss\n",
    "                seed_train_r2s[e] = train_r2\n",
    "                seed_val_r2s[e] = val_r2\n",
    "\n",
    "                writer.add_scalar(\"Loss/train\", train_loss, e)\n",
    "                writer.add_scalar(\"Loss/val\", val_loss, e)\n",
    "                writer.add_scalar(\"R2/train\", train_r2, e)\n",
    "                writer.add_scalar(\"R2/val\", val_r2, e)\n",
    "\n",
    "                # Update LR scheduler\n",
    "                if type(lr) == str:\n",
    "                    sched.step(val_loss)\n",
    "\n",
    "                # Update minimums\n",
    "                if val_loss < min_val_loss:\n",
    "                    val_activ_min_loss = val_activ.detach().clone().cpu().numpy()\n",
    "                    train_activ_min_loss = train_activ.detach().clone().cpu().numpy()\n",
    "                    min_val_loss = val_loss\n",
    "                if train_loss < min_train_loss:\n",
    "                    val_activ_mintrain_loss = val_activ.detach().clone().cpu().numpy()\n",
    "                    train_activ_mintrain_loss = (\n",
    "                        train_activ.detach().clone().cpu().numpy()\n",
    "                    )\n",
    "                    min_train_loss = train_loss\n",
    "\n",
    "            ### VERIFY EARLY STOP ###\n",
    "            # Is weird rn but basically we just want to record the first early stop activations, but since we also want the lowest validation error's activation we can't break out yet\n",
    "            if not early_stopping.early_stop:\n",
    "                early_stopping(val_loss, train_activ, val_activ)\n",
    "                if early_stopping.early_stop:\n",
    "                    ### PLOT EARLY STOP REPRESENTATION ###\n",
    "                    train_activ_graph_early_stop = (\n",
    "                        early_stopping.train_activ.cpu().numpy()\n",
    "                    )\n",
    "                    val_activ_graph_early_stop = early_stopping.val_activ.cpu().numpy()\n",
    "                    fig_pgt_train = plot_pred_vs_gt(\n",
    "                        y_train.cpu().numpy(),\n",
    "                        train_activ_graph_early_stop,\n",
    "                        title=\"Prédiction par rapport à la vérité (es) (entrainement)\",\n",
    "                    )\n",
    "\n",
    "                    fig_pgt_val = plot_pred_vs_gt(\n",
    "                        y_val.cpu().numpy(),\n",
    "                        val_activ_graph_early_stop,\n",
    "                        title=\"Prédiction par rapport à la vérité (es) (validation)\",\n",
    "                    )\n",
    "                    writer.add_figure(\"train_pred_vs_gt_es\", fig_pgt_train)\n",
    "                    writer.add_figure(\"val_pred_vs_gt_es\", fig_pgt_val)\n",
    "                    writer.flush()\n",
    "\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min loss) ###\n",
    "        fig_pgt_train = plot_pred_vs_gt(\n",
    "            y_train.cpu().numpy(),\n",
    "            train_activ_min_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (minval) (entrainement)\",\n",
    "        )\n",
    "\n",
    "        fig_pgt_val = plot_pred_vs_gt(\n",
    "            y_val.cpu().numpy(),\n",
    "            val_activ_min_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (minval) (validation)\",\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"train_pred_vs_gt_minval\", fig_pgt_train)\n",
    "        writer.add_figure(\"val_pred_vs_gt_minval\", fig_pgt_val)\n",
    "        ### PLOT PRED VS TRUE FOR THIS SEED  (min loss) ###\n",
    "        fig_pgt_train_mintrain = plot_pred_vs_gt(\n",
    "            y_train.cpu().numpy(),\n",
    "            train_activ_mintrain_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (mintrain) (entrainement)\",\n",
    "        )\n",
    "\n",
    "        fig_pgt_val_mintrain = plot_pred_vs_gt(\n",
    "            y_val.cpu().numpy(),\n",
    "            val_activ_mintrain_loss,\n",
    "            title=\"Prédiction par rapport à la vérité (mintrain) (validation)\",\n",
    "        )\n",
    "\n",
    "        writer.add_figure(\"train_pred_vs_gt_mintrain\", fig_pgt_train_mintrain)\n",
    "        writer.add_figure(\"val_pred_vs_gt_mintrain\", fig_pgt_val_mintrain)\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        train_losses.append(seed_train_losses)\n",
    "        val_losses.append(seed_val_losses)\n",
    "        train_r2s.append(seed_train_r2s)\n",
    "        val_r2s.append(seed_val_r2s)\n",
    "    ### PLOT AGGREGATE DATA FOR ALL SEEDS ###\n",
    "    # logdir = f\"runs/{exp_dir}/aggregate\"\n",
    "    # writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "    # ### PLOTS ###\n",
    "    # train_losses = np.array(train_losses)\n",
    "    # val_losses = np.array(val_losses)\n",
    "    # fig_loss = plot_losses(n_epochs, train_losses, val_losses)\n",
    "\n",
    "    # train_r2s = np.array(train_r2s)\n",
    "    # val_r2s = np.array(val_r2s)\n",
    "    # fig_r2 = plot_losses(n_epochs, train_r2s, val_r2s)\n",
    "\n",
    "    # writer.add_figure(\"losses\", fig_loss)\n",
    "    # writer.add_figure(\"r2s\", fig_r2)\n",
    "    \n",
    "    # writer.flush()\n",
    "    # writer.close()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    save_metrics(train_losses, f\"metrics/{exp_dir}/train_losses\")\n",
    "    save_metrics(val_losses, f\"metrics/{exp_dir}/val_losses\")\n",
    "    save_metrics(train_r2s, f\"metrics/{exp_dir}/train_r2s\")\n",
    "    save_metrics(val_r2s, f\"metrics/{exp_dir}/val_r2s\")\n",
    "\n",
    "    # print(f\"saved at runs/{exp_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 128, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 3, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': None, 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 32, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 1024, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['mse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "{'dataset': '1000_rx_100000_combis_10_patterns_25', 'width': 256, 'hidden': 5, 'n_obs': 20000, 'decay': 0, 'lr': 0.001, 'custom_layers': None, 'reweight': 'sqrt_inv', 'batch_size': 2048, 'dropout_rate': None, 'loss': ['rmse'], 'classif_thresh': None}\n",
      "Using label distribution smoothing\n",
      "training AE...\n",
      "AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for config in configs[49:]:\n",
    "    print(config)\n",
    "    run_config(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec05a26f12930ec341a8988c0de5d31e9cfb37c98ea12af5353828508f2c236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
